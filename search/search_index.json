{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"BasicUsage/","title":"Home","text":"<p>The code is divided into 2 classes</p> In\u00a0[\u00a0]: Copied! <pre>from xrai import Transform\nfrom xrai import Preparation\n\npreparation = Preparation(base_path=\"/mnt/DATA/ACAD/KPP_HiWi/\",\n                                        #   base_path=os.getcwd(),\n                                          file_name=\"distortions_final.xlsx\",\n                                          outcome=None,\n                                          outcome_list=[],\n                                          classed_splits=False,\n                                          outcome_to_features=[],\n                                          test_sets=10,\n                                          val_sets=5)\ntransformer = Transform(dataPrepared=preparation)\ntransformer.gen_plots()\n</pre> from xrai import Transform from xrai import Preparation  preparation = Preparation(base_path=\"/mnt/DATA/ACAD/KPP_HiWi/\",                                         #   base_path=os.getcwd(),                                           file_name=\"distortions_final.xlsx\",                                           outcome=None,                                           outcome_list=[],                                           classed_splits=False,                                           outcome_to_features=[],                                           test_sets=10,                                           val_sets=5) transformer = Transform(dataPrepared=preparation) transformer.gen_plots() <pre>Imported 0.3.2 version. Select nrows to a small number when running on huge datasets.\noutput = featurewiz(dataname, target, corr_limit=0.90, verbose=2, sep=',', \n\t\theader=0, test_data='',feature_engg='', category_encoders='',\n\t\tdask_xgboost_flag=False, nrows=None, skip_sulov=False, skip_xgboost=False)\nCreate new features via 'feature_engg' flag : ['interactions','groupby','target']\n\nwiz = FeatureWiz(verbose=1)\n        X_train_selected = wiz.fit_transform(X_train, y_train)\n        X_test_selected = wiz.transform(X_test)\n        wiz.features  ### provides a list of selected features ###            \n        \n/mnt/DATA/ACAD/KPP_HiWi/\n0      1002P07_03\n1      1002P07_25\n2      1005P07_04\n3      1005P07_05\n4      1005P07_10\n          ...    \n525    6803P17_13\n526    6805P17_03\n527    6841P17_03\n528    6898P17_04\n529    6898P17_07\nName: ID, Length: 530, dtype: object\n0\n0\n1\n2\n3\n4\n1\n0\n1\n2\n3\n4\n2\n0\n1\n2\n3\n4\n3\n0\n1\n2\n3\n4\n4\n0\n1\n2\n3\n4\n5\n0\n1\n2\n3\n4\n6\n0\n1\n2\n3\n4\n7\n0\n1\n2\n3\n4\n8\n0\n1\n2\n3\n4\n9\n0\n1\n2\n3\n4\n/mnt/DATA/ACAD/KPP_HiWi/\n0      1002P07_03\n1      1002P07_25\n2      1005P07_04\n3      1005P07_05\n4      1005P07_10\n          ...    \n525    6803P17_13\n526    6805P17_03\n527    6841P17_03\n528    6898P17_04\n529    6898P17_07\nName: ID, Length: 530, dtype: object\n0\n0\n1\n2\n3\n4\n1\n0\n1\n2\n3\n4\n2\n0\n1\n2\n3\n4\n3\n0\n1\n2\n3\n4\n4\n0\n1\n2\n3\n4\n5\n0\n1\n2\n3\n4\n6\n0\n1\n2\n3\n4\n7\n0\n1\n2\n3\n4\n8\n0\n1\n2\n3\n4\n9\n0\n1\n2\n3\n4\nwiz = FeatureWiz(verbose=1)\n        X_train_selected = wiz.fit_transform(X_train, y_train)\n        X_test_selected = wiz.transform(X_test)\n        wiz.features  ### provides a list of selected features ###            \n        \nTest fold: 0\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (381, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.272727272727273, 5: 1.3, 6: 1.363636363636364, 7: 1.4, 8: 1.409090909090909, 9: 1.454545454545455, 10: 1.545454545454545, 11: 1.6, 12: 1.636363636363636, 13: 1.7, 14: 1.727272727272727, 15: 1.818181818181818, 16: 1.909090909090909, 17: 1.954545454545455, 18: 2.0, 19: 2.090909090909091, 20: 2.181818181818182, 21: 2.272727272727273, 22: 2.363636363636364, 23: 2.4, 24: 2.454545454545455, 25: 2.545454545454545, 26: 2.575, 27: 2.636363636363636, 28: 2.666666666666667, 29: 2.7, 30: 2.727272727272727, 31: 2.772727272727273, 32: 2.818181818181818, 33: 2.909090909090909, 34: 3.0, 35: 3.090909090909091, 36: 3.181818181818182, 37: 3.272727272727273, 38: 3.363636363636364, 39: 3.454545454545455, 40: 3.545454545454545, 41: 3.636363636363636, 42: 3.727272727272727, 43: 3.818181818181818, 44: 3.909090909090909, 45: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 2 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 2 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 2 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 2 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 4 seconds\n    Completed XGBoost feature selection in 4 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative', 'mentalfiltering']\nTotal Time taken for featurewiz selection = 13 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 13 second(s)\nValidation Fold: 0\n{'best_params': {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(5), 'lambda_l2': np.int64(1)}, 'best_iter': np.int64(44), 'best_score': np.float64(0.4963003276308127)}\n[GPBoost] [Info] Total Bins 1821\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.173863\n[GPBoost] [Info] Start training from score 2.173863\nGPB_nrmse: 0.7972976525943888\nGPB_r: 0.6546566725382262\nGPB_params: {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(5), 'lambda_l2': np.int64(1)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -643.5803651276082 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1034.7529651851846 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1129.1932011608187 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1174.4242813293667 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1182.4705520754997 at iteration 5.\n</pre> <pre>merf_nrmse: 0.593851686729745\nmerf_r: 0.8118589440660336\n[GPBoost] [Info] Total Bins 1821\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.173863\n[GPBoost] [Info] Start training from score 2.173863\nSuper_nrmse: 0.7070279762405152\nSuper_r: 0.7210667929379224\nSuper_params: {'model__C': 9, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (381, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.454545454545455, 9: 1.545454545454545, 10: 1.636363636363636, 11: 1.666666666666667, 12: 1.727272727272727, 13: 1.818181818181818, 14: 1.909090909090909, 15: 1.954545454545455, 16: 2.0, 17: 2.090909090909091, 18: 2.181818181818182, 19: 2.272727272727273, 20: 2.363636363636364, 21: 2.4, 22: 2.454545454545455, 23: 2.545454545454545, 24: 2.575, 25: 2.636363636363636, 26: 2.666666666666667, 27: 2.7, 28: 2.727272727272727, 29: 2.772727272727273, 30: 2.818181818181818, 31: 2.909090909090909, 32: 3.0, 33: 3.090909090909091, 34: 3.181818181818182, 35: 3.272727272727273, 36: 3.363636363636364, 37: 3.454545454545455, 38: 3.545454545454545, 39: 3.636363636363636, 40: 3.727272727272727, 41: 3.818181818181818, 42: 3.909090909090909, 43: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 2 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 2 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 2 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 2 seconds\n        Selected: ['negative', 'positive', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 3 seconds\n    Completed XGBoost feature selection in 3 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nTotal Time taken for featurewiz selection = 12 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 11 second(s)\nValidation Fold: 1\n{'best_params': {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}, 'best_iter': np.int64(18), 'best_score': np.float64(0.49318472798072044)}\n[GPBoost] [Info] Total Bins 1819\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.190334\n[GPBoost] [Info] Start training from score 2.190334\nGPB_nrmse: 0.7507065812768388\nGPB_r: 0.7125498301771747\nGPB_params: {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -636.6680146959495 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1038.076091144851 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1150.9003816536758 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1194.8025090849646 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1195.9444414514162 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6334829256181133\nmerf_r: 0.7754778918043439\n[GPBoost] [Info] Total Bins 1819\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.190334\n[GPBoost] [Info] Start training from score 2.190334\nSuper_nrmse: 0.6377007820895492\nSuper_r: 0.7785873573456163\nSuper_params: {'model__C': 2, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.4, 9: 1.409090909090909, 10: 1.454545454545455, 11: 1.545454545454545, 12: 1.6, 13: 1.636363636363636, 14: 1.666666666666667, 15: 1.7, 16: 1.727272727272727, 17: 1.818181818181818, 18: 1.909090909090909, 19: 1.954545454545455, 20: 2.0, 21: 2.090909090909091, 22: 2.181818181818182, 23: 2.272727272727273, 24: 2.363636363636364, 25: 2.4, 26: 2.454545454545455, 27: 2.545454545454545, 28: 2.575, 29: 2.636363636363636, 30: 2.666666666666667, 31: 2.7, 32: 2.727272727272727, 33: 2.818181818181818, 34: 2.909090909090909, 35: 3.0, 36: 3.090909090909091, 37: 3.181818181818182, 38: 3.272727272727273, 39: 3.363636363636364, 40: 3.454545454545455, 41: 3.545454545454545, 42: 3.636363636363636, 43: 3.727272727272727, 44: 3.818181818181818, 45: 3.909090909090909, 46: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 2 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 2 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 2 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 3 seconds\n    Completed XGBoost feature selection in 3 seconds\nSelected 14 important features:\n['comparison', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'I_talk', 'words']\nTotal Time taken for featurewiz selection = 12 seconds\nOutput contains a list of 14 important features and a train dataframe\n    Time taken to create entire pipeline = 12 second(s)\nValidation Fold: 2\n{'best_params': {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}, 'best_iter': np.int64(8), 'best_score': np.float64(0.5179408481687615)}\n[GPBoost] [Info] Total Bins 1652\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 14\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.188575\n[GPBoost] [Info] Start training from score 2.188575\nGPB_nrmse: 0.7324773254454239\nGPB_r: 0.7076633826913244\nGPB_params: {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -634.2260002254694 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1037.1917092864417 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1153.9310513256062 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1190.0046781905216 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1175.1183599061524 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6410068500255889\nmerf_r: 0.7706203029260602\n[GPBoost] [Info] Total Bins 1652\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 14\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.188575\n[GPBoost] [Info] Start training from score 2.188575\nSuper_nrmse: 0.6766820319576671\nSuper_r: 0.7412169087807814\nSuper_params: {'model__C': 10, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'scale', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.4, 9: 1.409090909090909, 10: 1.454545454545455, 11: 1.545454545454545, 12: 1.6, 13: 1.636363636363636, 14: 1.666666666666667, 15: 1.7, 16: 1.727272727272727, 17: 1.818181818181818, 18: 1.909090909090909, 19: 2.0, 20: 2.090909090909091, 21: 2.181818181818182, 22: 2.272727272727273, 23: 2.363636363636364, 24: 2.454545454545455, 25: 2.545454545454545, 26: 2.636363636363636, 27: 2.727272727272727, 28: 2.772727272727273, 29: 2.818181818181818, 30: 2.909090909090909, 31: 3.0, 32: 3.090909090909091, 33: 3.181818181818182, 34: 3.272727272727273, 35: 3.363636363636364, 36: 3.454545454545455, 37: 3.545454545454545, 38: 3.636363636363636, 39: 3.727272727272727, 40: 3.818181818181818, 41: 3.909090909090909, 42: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 2 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 2 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 3 seconds\n    Completed XGBoost feature selection in 3 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative', 'mentalfiltering', 'I_talk']\nTotal Time taken for featurewiz selection = 14 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 13 second(s)\nValidation Fold: 3\n{'best_params': {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(1)}, 'best_iter': np.int64(110), 'best_score': np.float64(0.4801740525991396)}\n[GPBoost] [Info] Total Bins 1820\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.145867\n[GPBoost] [Info] Start training from score 2.145867\nGPB_nrmse: 0.639268631225604\nGPB_r: 0.7664417308103818\nGPB_params: {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(1)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -640.0865800971171 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1047.1386739576265 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1167.281823044278 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1207.4715688275842 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1213.1580696138644 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6541031916426564\nmerf_r: 0.7583735758064417\n[GPBoost] [Info] Total Bins 1820\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.145867\n[GPBoost] [Info] Start training from score 2.145867\nSuper_nrmse: 0.7177746244675094\nSuper_r: 0.6934716361443467\nSuper_params: {'model__C': 3, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.181818181818182, 3: 1.2, 4: 1.272727272727273, 5: 1.3, 6: 1.363636363636364, 7: 1.4, 8: 1.409090909090909, 9: 1.454545454545455, 10: 1.545454545454545, 11: 1.6, 12: 1.636363636363636, 13: 1.666666666666667, 14: 1.7, 15: 1.727272727272727, 16: 1.818181818181818, 17: 1.909090909090909, 18: 1.954545454545455, 19: 2.0, 20: 2.090909090909091, 21: 2.181818181818182, 22: 2.272727272727273, 23: 2.363636363636364, 24: 2.4, 25: 2.454545454545455, 26: 2.545454545454545, 27: 2.575, 28: 2.636363636363636, 29: 2.666666666666667, 30: 2.7, 31: 2.727272727272727, 32: 2.772727272727273, 33: 2.818181818181818, 34: 2.909090909090909, 35: 3.0, 36: 3.090909090909091, 37: 3.181818181818182, 38: 3.272727272727273, 39: 3.363636363636364, 40: 3.454545454545455, 41: 3.545454545454545, 42: 3.636363636363636, 43: 3.727272727272727, 44: 3.818181818181818, 45: 3.909090909090909, 46: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 2 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 4 seconds\n    Completed XGBoost feature selection in 4 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative', 'I_talk']\nTotal Time taken for featurewiz selection = 15 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 14 second(s)\nValidation Fold: 4\n{'best_params': {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(0)}, 'best_iter': np.int64(121), 'best_score': np.float64(0.49343804701985067)}\n[GPBoost] [Info] Total Bins 1830\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.175414\n[GPBoost] [Info] Start training from score 2.175414\nGPB_nrmse: 0.5690581225711474\nGPB_r: 0.8203779687733643\nGPB_params: {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(0)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -635.3038395165618 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1032.3642749690541 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1145.2886403772573 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1169.0819258563022 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1176.088816943548 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6146494679978951\nmerf_r: 0.7936934106932793\n[GPBoost] [Info] Total Bins 1830\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.175414\n[GPBoost] [Info] Start training from score 2.175414\nSuper_nrmse: 0.6656846315502986\nSuper_r: 0.7446627028676234\nSuper_params: {'model__C': 2, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nmerf mean r: 0.7820048250592317\nmerfmean nrmse: 0.6274188244027997\nsuper mean r: 0.7358010796152581\nsupermean nrmse: 0.6809740092611078\ngpb mean r: 0.7323379169980944\ngpbmean nrmse: 0.6977616626226806\n{'merf': np.float64(0.7820048250592317), 'super': np.float64(0.7358010796152581), 'gpb': np.float64(0.7323379169980944)}\n{'merf': np.float64(41.69834830694014), 'super': np.float64(26.187227292087517), 'gpb': np.float64(12.966751496331401)}\nTest fold: 1\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (381, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.181818181818182, 3: 1.272727272727273, 4: 1.3, 5: 1.363636363636364, 6: 1.4, 7: 1.409090909090909, 8: 1.454545454545455, 9: 1.545454545454545, 10: 1.6, 11: 1.636363636363636, 12: 1.7, 13: 1.727272727272727, 14: 1.818181818181818, 15: 1.909090909090909, 16: 2.0, 17: 2.090909090909091, 18: 2.1, 19: 2.181818181818182, 20: 2.272727272727273, 21: 2.363636363636364, 22: 2.4, 23: 2.454545454545455, 24: 2.545454545454545, 25: 2.575, 26: 2.636363636363636, 27: 2.666666666666667, 28: 2.7, 29: 2.727272727272727, 30: 2.772727272727273, 31: 2.818181818181818, 32: 2.909090909090909, 33: 3.0, 34: 3.090909090909091, 35: 3.181818181818182, 36: 3.272727272727273, 37: 3.363636363636364, 38: 3.454545454545455, 39: 3.545454545454545, 40: 3.636363636363636, 41: 3.727272727272727, 42: 3.818181818181818, 43: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (2) highly correlated variables:\n    ['I_talk', 'neutral']\n    Following (18) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\nCompleted SULOV. 18 features selected\nTime taken for SULOV method = 0 seconds\nFinally 18 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\n            Time taken for regular XGBoost feature selection = 2 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\n            Time taken for regular XGBoost feature selection = 2 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['negative', 'words']\n            Time taken for regular XGBoost feature selection = 4 seconds\n    Completed XGBoost feature selection in 4 seconds\nSelected 15 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\nTotal Time taken for featurewiz selection = 14 seconds\nOutput contains a list of 15 important features and a train dataframe\n    Time taken to create entire pipeline = 14 second(s)\nValidation Fold: 0\n{'best_params': {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(0)}, 'best_iter': np.int64(5), 'best_score': np.float64(0.5036108526252121)}\n[GPBoost] [Info] Total Bins 1690\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.173195\n[GPBoost] [Info] Start training from score 2.173195\nGPB_nrmse: 0.8128788793713513\nGPB_r: 0.649896683249342\nGPB_params: {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(0)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -641.613288158452 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1029.8284091058574 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1137.731108867781 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1163.6992180396915 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1166.9522181636194 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6779325026431068\nmerf_r: 0.7344924995499388\n[GPBoost] [Info] Total Bins 1690\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.173195\n[GPBoost] [Info] Start training from score 2.173195\nSuper_nrmse: 0.6997971453007815\nSuper_r: 0.7220352710355105\nSuper_params: {'model__C': 10, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'scale', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (381, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.454545454545455, 9: 1.545454545454545, 10: 1.636363636363636, 11: 1.7, 12: 1.727272727272727, 13: 1.818181818181818, 14: 1.909090909090909, 15: 2.0, 16: 2.090909090909091, 17: 2.1, 18: 2.181818181818182, 19: 2.272727272727273, 20: 2.363636363636364, 21: 2.4, 22: 2.454545454545455, 23: 2.545454545454545, 24: 2.575, 25: 2.636363636363636, 26: 2.7, 27: 2.727272727272727, 28: 2.772727272727273, 29: 2.818181818181818, 30: 2.909090909090909, 31: 3.0, 32: 3.090909090909091, 33: 3.181818181818182, 34: 3.272727272727273, 35: 3.363636363636364, 36: 3.454545454545455, 37: 3.545454545454545, 38: 3.636363636363636, 39: 3.727272727272727, 40: 3.818181818181818, 41: 3.909090909090909, 42: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 2 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 3 seconds\n    Completed XGBoost feature selection in 3 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative', 'mentalfiltering']\nTotal Time taken for featurewiz selection = 14 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 13 second(s)\nValidation Fold: 1\n{'best_params': {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}, 'best_iter': np.int64(106), 'best_score': np.float64(0.5146502665282345)}\n[GPBoost] [Info] Total Bins 1819\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.165193\n[GPBoost] [Info] Start training from score 2.165193\nGPB_nrmse: 0.5856334428348181\nGPB_r: 0.8085239472535213\nGPB_params: {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -633.9559530806176 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1021.3379145283229 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1128.7672096904244 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1154.336397615016 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1149.5070409562063 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6161218914651039\nmerf_r: 0.7907784688639625\n[GPBoost] [Info] Total Bins 1819\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.165193\n[GPBoost] [Info] Start training from score 2.165193\nSuper_nrmse: 0.6709071660981064\nSuper_r: 0.7403364818530541\nSuper_params: {'model__C': 8, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.4, 9: 1.409090909090909, 10: 1.454545454545455, 11: 1.545454545454545, 12: 1.6, 13: 1.636363636363636, 14: 1.727272727272727, 15: 1.818181818181818, 16: 1.909090909090909, 17: 2.0, 18: 2.090909090909091, 19: 2.181818181818182, 20: 2.272727272727273, 21: 2.363636363636364, 22: 2.4, 23: 2.454545454545455, 24: 2.545454545454545, 25: 2.575, 26: 2.636363636363636, 27: 2.666666666666667, 28: 2.7, 29: 2.727272727272727, 30: 2.818181818181818, 31: 2.909090909090909, 32: 3.0, 33: 3.090909090909091, 34: 3.181818181818182, 35: 3.272727272727273, 36: 3.363636363636364, 37: 3.454545454545455, 38: 3.545454545454545, 39: 3.636363636363636, 40: 3.727272727272727, 41: 3.818181818181818, 42: 3.909090909090909, 43: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (2) highly correlated variables:\n    ['I_talk', 'neutral']\n    Following (18) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\nCompleted SULOV. 18 features selected\nTime taken for SULOV method = 0 seconds\nFinally 18 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 2 seconds\n        Selected: ['personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['words', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n    Completed XGBoost feature selection in 3 seconds\nSelected 15 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\nTotal Time taken for featurewiz selection = 14 seconds\nOutput contains a list of 15 important features and a train dataframe\n    Time taken to create entire pipeline = 14 second(s)\nValidation Fold: 2\n{'best_params': {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}, 'best_iter': np.int64(18), 'best_score': np.float64(0.4733814733620686)}\n[GPBoost] [Info] Total Bins 1688\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.201370\n[GPBoost] [Info] Start training from score 2.201370\nGPB_nrmse: 0.8276324800909842\nGPB_r: 0.6107202128306523\nGPB_params: {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -646.6696865357853 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1057.142173247593 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1182.33193410297 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1204.5351827276224 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1212.587778032102 at iteration 5.\n</pre> <pre>merf_nrmse: 0.7186072266201359\nmerf_r: 0.693323781514741\n[GPBoost] [Info] Total Bins 1688\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.201370\n[GPBoost] [Info] Start training from score 2.201370\nSuper_nrmse: 0.7394009945687552\nSuper_r: 0.6771776687239528\nSuper_params: {'model__C': 1, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.4, 9: 1.409090909090909, 10: 1.454545454545455, 11: 1.545454545454545, 12: 1.6, 13: 1.636363636363636, 14: 1.7, 15: 1.727272727272727, 16: 1.818181818181818, 17: 1.909090909090909, 18: 2.0, 19: 2.090909090909091, 20: 2.1, 21: 2.181818181818182, 22: 2.272727272727273, 23: 2.363636363636364, 24: 2.4, 25: 2.454545454545455, 26: 2.545454545454545, 27: 2.575, 28: 2.636363636363636, 29: 2.666666666666667, 30: 2.727272727272727, 31: 2.772727272727273, 32: 2.818181818181818, 33: 2.909090909090909, 34: 3.0, 35: 3.090909090909091, 36: 3.181818181818182, 37: 3.272727272727273, 38: 3.363636363636364, 39: 3.454545454545455, 40: 3.545454545454545, 41: 3.636363636363636, 42: 3.727272727272727, 43: 3.818181818181818, 44: 3.909090909090909, 45: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['negative', 'positive', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 4 seconds\n    Completed XGBoost feature selection in 4 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nTotal Time taken for featurewiz selection = 15 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 15 second(s)\nValidation Fold: 3\n{'best_params': {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}, 'best_iter': np.int64(14), 'best_score': np.float64(0.4903096298335027)}\n[GPBoost] [Info] Total Bins 1826\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.177239\n[GPBoost] [Info] Start training from score 2.177239\nGPB_nrmse: 0.724485751163018\nGPB_r: 0.7214242126797079\nGPB_params: {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -629.7420074693069 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1013.9097790222305 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1132.3254871364593 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1161.0397582356368 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1162.485643662257 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6557635192404022\nmerf_r: 0.7713453422450418\n[GPBoost] [Info] Total Bins 1826\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.177239\n[GPBoost] [Info] Start training from score 2.177239\nSuper_nrmse: 0.6406214811672067\nSuper_r: 0.7841206786803225\nSuper_params: {'model__C': 7, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.4, 9: 1.409090909090909, 10: 1.454545454545455, 11: 1.545454545454545, 12: 1.6, 13: 1.636363636363636, 14: 1.7, 15: 1.727272727272727, 16: 1.818181818181818, 17: 1.909090909090909, 18: 2.0, 19: 2.090909090909091, 20: 2.1, 21: 2.181818181818182, 22: 2.272727272727273, 23: 2.363636363636364, 24: 2.454545454545455, 25: 2.545454545454545, 26: 2.636363636363636, 27: 2.666666666666667, 28: 2.7, 29: 2.727272727272727, 30: 2.772727272727273, 31: 2.818181818181818, 32: 2.909090909090909, 33: 3.0, 34: 3.090909090909091, 35: 3.181818181818182, 36: 3.272727272727273, 37: 3.363636363636364, 38: 3.454545454545455, 39: 3.545454545454545, 40: 3.636363636363636, 41: 3.727272727272727, 42: 3.818181818181818, 43: 3.909090909090909, 44: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 2 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 2 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 4 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 4 seconds\n    Completed XGBoost feature selection in 4 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nTotal Time taken for featurewiz selection = 15 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 14 second(s)\nValidation Fold: 4\n{'best_params': {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(0)}, 'best_iter': np.int64(9), 'best_score': np.float64(0.5039808569634627)}\n[GPBoost] [Info] Total Bins 1827\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.181041\n[GPBoost] [Info] Start training from score 2.181041\nGPB_nrmse: 0.7656420168133676\nGPB_r: 0.677032378381386\nGPB_params: {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(0)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -636.9373086392781 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1035.1054946351514 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1143.3262343682034 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1180.176705555064 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1173.148157472121 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6863044279408949\nmerf_r: 0.7306444614570134\n[GPBoost] [Info] Total Bins 1827\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.181041\n[GPBoost] [Info] Start training from score 2.181041\nSuper_nrmse: 0.7032531211488537\nSuper_r: 0.7270055698962223\nSuper_params: {'model__C': 10, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nmerf mean r: 0.7441169107261395\nmerfmean nrmse: 0.6709459135819287\nsuper mean r: 0.7301351340378124\nsupermean nrmse: 0.6907959816567407\ngpb mean r: 0.6935194868789218\ngpbmean nrmse: 0.7432545140547079\n{'merf': np.float64(0.7441169107261395), 'super': np.float64(0.7301351340378124), 'gpb': np.float64(0.6935194868789218)}\n{'merf': np.float64(21.899498795051816), 'super': np.float64(21.25793523153728), 'gpb': np.float64(10.21757282193462)}\nTest fold: 2\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (381, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.181818181818182, 3: 1.2, 4: 1.272727272727273, 5: 1.3, 6: 1.363636363636364, 7: 1.409090909090909, 8: 1.454545454545455, 9: 1.545454545454545, 10: 1.636363636363636, 11: 1.666666666666667, 12: 1.7, 13: 1.727272727272727, 14: 1.818181818181818, 15: 1.909090909090909, 16: 2.0, 17: 2.090909090909091, 18: 2.1, 19: 2.181818181818182, 20: 2.272727272727273, 21: 2.363636363636364, 22: 2.4, 23: 2.454545454545455, 24: 2.545454545454545, 25: 2.575, 26: 2.636363636363636, 27: 2.666666666666667, 28: 2.7, 29: 2.727272727272727, 30: 2.772727272727273, 31: 2.818181818181818, 32: 2.909090909090909, 33: 3.0, 34: 3.090909090909091, 35: 3.181818181818182, 36: 3.272727272727273, 37: 3.363636363636364, 38: 3.454545454545455, 39: 3.545454545454545, 40: 3.636363636363636, 41: 3.727272727272727, 42: 3.818181818181818, 43: 3.909090909090909, 44: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 4 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 5 seconds\n    Completed XGBoost feature selection in 5 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nTotal Time taken for featurewiz selection = 17 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 17 second(s)\nValidation Fold: 0\n{'best_params': {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(1)}, 'best_iter': np.int64(12), 'best_score': np.float64(0.5198089822955214)}\n[GPBoost] [Info] Total Bins 1822\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.172439\n[GPBoost] [Info] Start training from score 2.172439\nGPB_nrmse: 0.6563977612780113\nGPB_r: 0.7585470296140279\nGPB_params: {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(1)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -637.6270913882316 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1023.543474290116 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1131.788566417571 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1153.0677408900704 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1149.8602910023087 at iteration 5.\n</pre> <pre>merf_nrmse: 0.5352630085692607\nmerf_r: 0.8621374239319727\n[GPBoost] [Info] Total Bins 1822\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.172439\n[GPBoost] [Info] Start training from score 2.172439\nSuper_nrmse: 0.5305445655641235\nSuper_r: 0.8537918984322194\nSuper_params: {'model__C': 5, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (381, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.181818181818182, 3: 1.2, 4: 1.272727272727273, 5: 1.3, 6: 1.363636363636364, 7: 1.4, 8: 1.409090909090909, 9: 1.454545454545455, 10: 1.545454545454545, 11: 1.6, 12: 1.636363636363636, 13: 1.666666666666667, 14: 1.7, 15: 1.727272727272727, 16: 1.818181818181818, 17: 1.909090909090909, 18: 1.954545454545455, 19: 2.0, 20: 2.090909090909091, 21: 2.181818181818182, 22: 2.272727272727273, 23: 2.363636363636364, 24: 2.454545454545455, 25: 2.545454545454545, 26: 2.636363636363636, 27: 2.666666666666667, 28: 2.727272727272727, 29: 2.818181818181818, 30: 2.909090909090909, 31: 3.0, 32: 3.090909090909091, 33: 3.181818181818182, 34: 3.272727272727273, 35: 3.363636363636364, 36: 3.454545454545455, 37: 3.545454545454545, 38: 3.636363636363636, 39: 3.727272727272727, 40: 3.818181818181818, 41: 3.909090909090909, 42: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['negative', 'positive', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 4 seconds\n    Completed XGBoost feature selection in 4 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'I_talk', 'words']\nTotal Time taken for featurewiz selection = 16 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 15 second(s)\nValidation Fold: 1\n{'best_params': {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(0)}, 'best_iter': np.int64(1), 'best_score': np.float64(0.5029706784481072)}\n[GPBoost] [Info] Total Bins 1818\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.197845\n[GPBoost] [Info] Start training from score 2.197845\nGPB_nrmse: 0.844898922979141\nGPB_r: 0.6430124317590935\nGPB_params: {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(0)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -631.9093962820875 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1016.9137564000816 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1117.9821445933162 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1143.8765008986484 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1145.1324411832934 at iteration 5.\n</pre> <pre>merf_nrmse: 0.623806808332229\nmerf_r: 0.7790126888010086\n[GPBoost] [Info] Total Bins 1818\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.197845\n[GPBoost] [Info] Start training from score 2.197845\nSuper_nrmse: 0.6499534663456075\nSuper_r: 0.7590038770004126\nSuper_params: {'model__C': 2, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.181818181818182, 3: 1.2, 4: 1.272727272727273, 5: 1.3, 6: 1.363636363636364, 7: 1.4, 8: 1.409090909090909, 9: 1.454545454545455, 10: 1.545454545454545, 11: 1.6, 12: 1.636363636363636, 13: 1.666666666666667, 14: 1.7, 15: 1.727272727272727, 16: 1.818181818181818, 17: 1.909090909090909, 18: 1.954545454545455, 19: 2.0, 20: 2.090909090909091, 21: 2.1, 22: 2.181818181818182, 23: 2.272727272727273, 24: 2.363636363636364, 25: 2.4, 26: 2.454545454545455, 27: 2.545454545454545, 28: 2.575, 29: 2.636363636363636, 30: 2.7, 31: 2.727272727272727, 32: 2.772727272727273, 33: 2.818181818181818, 34: 2.909090909090909, 35: 3.0, 36: 3.090909090909091, 37: 3.181818181818182, 38: 3.272727272727273, 39: 3.363636363636364, 40: 3.454545454545455, 41: 3.545454545454545, 42: 3.636363636363636, 43: 3.727272727272727, 44: 3.818181818181818, 45: 3.909090909090909, 46: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (2) highly correlated variables:\n    ['I_talk', 'neutral']\n    Following (18) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\nCompleted SULOV. 18 features selected\nTime taken for SULOV method = 1 seconds\nFinally 18 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\n            Time taken for regular XGBoost feature selection = 4 seconds\n        Selected: ['negative', 'words']\n            Time taken for regular XGBoost feature selection = 5 seconds\n    Completed XGBoost feature selection in 5 seconds\nSelected 13 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative', 'minimization']\nTotal Time taken for featurewiz selection = 19 seconds\nOutput contains a list of 13 important features and a train dataframe\n    Time taken to create entire pipeline = 19 second(s)\nValidation Fold: 2\n{'best_params': {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}, 'best_iter': np.int64(10), 'best_score': np.float64(0.4879949263606994)}\n[GPBoost] [Info] Total Bins 1540\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 13\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.196944\n[GPBoost] [Info] Start training from score 2.196944\nGPB_nrmse: 0.8455110324723604\nGPB_r: 0.6026072782693879\nGPB_params: {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -642.2436263611062 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1036.2659999342384 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1143.2173878477574 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1167.0945179132816 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1171.822188242092 at iteration 5.\n</pre> <pre>merf_nrmse: 0.735115577374195\nmerf_r: 0.6795862648273924\n[GPBoost] [Info] Total Bins 1540\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 13\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.196944\n[GPBoost] [Info] Start training from score 2.196944\nSuper_nrmse: 0.755953464382442\nSuper_r: 0.6698682078025796\nSuper_params: {'model__C': 8, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.181818181818182, 3: 1.2, 4: 1.272727272727273, 5: 1.3, 6: 1.363636363636364, 7: 1.4, 8: 1.454545454545455, 9: 1.545454545454545, 10: 1.6, 11: 1.636363636363636, 12: 1.666666666666667, 13: 1.727272727272727, 14: 1.818181818181818, 15: 1.909090909090909, 16: 1.954545454545455, 17: 2.0, 18: 2.090909090909091, 19: 2.1, 20: 2.181818181818182, 21: 2.272727272727273, 22: 2.363636363636364, 23: 2.4, 24: 2.454545454545455, 25: 2.545454545454545, 26: 2.575, 27: 2.636363636363636, 28: 2.666666666666667, 29: 2.7, 30: 2.727272727272727, 31: 2.772727272727273, 32: 2.818181818181818, 33: 2.909090909090909, 34: 3.0, 35: 3.090909090909091, 36: 3.181818181818182, 37: 3.272727272727273, 38: 3.363636363636364, 39: 3.454545454545455, 40: 3.545454545454545, 41: 3.636363636363636, 42: 3.727272727272727, 43: 3.818181818181818, 44: 3.909090909090909, 45: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (2) highly correlated variables:\n    ['I_talk', 'neutral']\n    Following (18) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\nCompleted SULOV. 18 features selected\nTime taken for SULOV method = 0 seconds\nFinally 18 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['negative', 'words']\n            Time taken for regular XGBoost feature selection = 4 seconds\n    Completed XGBoost feature selection in 4 seconds\nSelected 15 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\nTotal Time taken for featurewiz selection = 16 seconds\nOutput contains a list of 15 important features and a train dataframe\n    Time taken to create entire pipeline = 15 second(s)\nValidation Fold: 3\n{'best_params': {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(0)}, 'best_iter': np.int64(66), 'best_score': np.float64(0.47936876680880686)}\n[GPBoost] [Info] Total Bins 1702\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.184125\n[GPBoost] [Info] Start training from score 2.184125\nGPB_nrmse: 0.7204937815626864\nGPB_r: 0.7052428027450455\nGPB_params: {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(0)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -648.7955204912131 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1042.5456259215682 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1154.4183830165828 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1168.1505886931848 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1166.5735629537342 at iteration 5.\n</pre> <pre>merf_nrmse: 0.718295651298681\nmerf_r: 0.6949395040709943\n[GPBoost] [Info] Total Bins 1702\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.184125\n[GPBoost] [Info] Start training from score 2.184125\nSuper_nrmse: 0.7577114493282915\nSuper_r: 0.6628033251819897\nSuper_params: {'model__C': 8, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.181818181818182, 3: 1.272727272727273, 4: 1.3, 5: 1.363636363636364, 6: 1.4, 7: 1.409090909090909, 8: 1.454545454545455, 9: 1.545454545454545, 10: 1.6, 11: 1.636363636363636, 12: 1.7, 13: 1.727272727272727, 14: 1.818181818181818, 15: 1.909090909090909, 16: 1.954545454545455, 17: 2.0, 18: 2.090909090909091, 19: 2.1, 20: 2.181818181818182, 21: 2.272727272727273, 22: 2.363636363636364, 23: 2.4, 24: 2.454545454545455, 25: 2.545454545454545, 26: 2.575, 27: 2.636363636363636, 28: 2.666666666666667, 29: 2.7, 30: 2.727272727272727, 31: 2.772727272727273, 32: 2.818181818181818, 33: 2.909090909090909, 34: 3.0, 35: 3.090909090909091, 36: 3.181818181818182, 37: 3.272727272727273, 38: 3.363636363636364, 39: 3.454545454545455, 40: 3.545454545454545, 41: 3.636363636363636, 42: 3.727272727272727, 43: 3.818181818181818, 44: 3.909090909090909, 45: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 4 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['negative', 'positive', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 4 seconds\n    Completed XGBoost feature selection in 4 seconds\nSelected 14 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words', 'I_talk']\nTotal Time taken for featurewiz selection = 17 seconds\nOutput contains a list of 14 important features and a train dataframe\n    Time taken to create entire pipeline = 17 second(s)\nValidation Fold: 4\n{'best_params': {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(0)}, 'best_iter': np.int64(7), 'best_score': np.float64(0.516471147884517)}\n[GPBoost] [Info] Total Bins 1667\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 14\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.192208\n[GPBoost] [Info] Start training from score 2.192208\nGPB_nrmse: 0.7531299839368647\nGPB_r: 0.6585854505357294\nGPB_params: {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(0)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -641.7334975683436 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1040.0059637693605 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1148.5937698157252 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1169.6462989170896 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1165.9719065986408 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6496930415332616\nmerf_r: 0.7684006029460169\n[GPBoost] [Info] Total Bins 1667\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 14\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.192208\n[GPBoost] [Info] Start training from score 2.192208\nSuper_nrmse: 0.6523612931603316\nSuper_r: 0.7568889825046692\nSuper_params: {'model__C': 6, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nmerf mean r: 0.7568152969154769\nmerfmean nrmse: 0.6524348174215255\nsuper mean r: 0.7404712581843741\nsupermean nrmse: 0.6693048477561592\ngpb mean r: 0.6735989985846567\ngpbmean nrmse: 0.7640862964458128\n{'merf': np.float64(0.7568152969154769), 'super': np.float64(0.7404712581843741), 'gpb': np.float64(0.6735989985846567)}\n{'merf': np.float64(11.537936347896906), 'super': np.float64(10.584282978908783), 'gpb': np.float64(12.54382243498477)}\nTest fold: 3\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (381, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.181818181818182, 3: 1.272727272727273, 4: 1.3, 5: 1.363636363636364, 6: 1.4, 7: 1.409090909090909, 8: 1.454545454545455, 9: 1.545454545454545, 10: 1.6, 11: 1.636363636363636, 12: 1.7, 13: 1.727272727272727, 14: 1.818181818181818, 15: 1.909090909090909, 16: 1.954545454545455, 17: 2.0, 18: 2.090909090909091, 19: 2.1, 20: 2.181818181818182, 21: 2.272727272727273, 22: 2.363636363636364, 23: 2.454545454545455, 24: 2.545454545454545, 25: 2.575, 26: 2.636363636363636, 27: 2.666666666666667, 28: 2.7, 29: 2.727272727272727, 30: 2.772727272727273, 31: 2.818181818181818, 32: 2.909090909090909, 33: 3.0, 34: 3.090909090909091, 35: 3.181818181818182, 36: 3.272727272727273, 37: 3.363636363636364, 38: 3.454545454545455, 39: 3.545454545454545, 40: 3.636363636363636, 41: 3.727272727272727, 42: 3.818181818181818, 43: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 1 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 4 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 4 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 5 seconds\n    Completed XGBoost feature selection in 5 seconds\nSelected 15 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative', 'I_talk']\nTotal Time taken for featurewiz selection = 19 seconds\nOutput contains a list of 15 important features and a train dataframe\n    Time taken to create entire pipeline = 19 second(s)\nValidation Fold: 0\n{'best_params': {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(0)}, 'best_iter': np.int64(94), 'best_score': np.float64(0.5072123974088718)}\n[GPBoost] [Info] Total Bins 1753\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.200181\n[GPBoost] [Info] Start training from score 2.200181\nGPB_nrmse: 0.6074196482320889\nGPB_r: 0.7999954124883059\nGPB_params: {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(0)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -627.6979811247255 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1012.4832451125515 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1123.6287998422656 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1149.4375166786988 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1149.2104819198814 at iteration 5.\n</pre> <pre>merf_nrmse: 0.579772838165164\nmerf_r: 0.8270487554039827\n[GPBoost] [Info] Total Bins 1753\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.200181\n[GPBoost] [Info] Start training from score 2.200181\nSuper_nrmse: 0.5915240565495629\nSuper_r: 0.8121142057572103\nSuper_params: {'model__C': 10, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'scale', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (381, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.272727272727273, 5: 1.3, 6: 1.363636363636364, 7: 1.454545454545455, 8: 1.545454545454545, 9: 1.636363636363636, 10: 1.666666666666667, 11: 1.727272727272727, 12: 1.818181818181818, 13: 1.909090909090909, 14: 2.0, 15: 2.090909090909091, 16: 2.1, 17: 2.181818181818182, 18: 2.272727272727273, 19: 2.363636363636364, 20: 2.454545454545455, 21: 2.545454545454545, 22: 2.575, 23: 2.636363636363636, 24: 2.7, 25: 2.727272727272727, 26: 2.772727272727273, 27: 2.818181818181818, 28: 2.909090909090909, 29: 3.0, 30: 3.090909090909091, 31: 3.181818181818182, 32: 3.272727272727273, 33: 3.363636363636364, 34: 3.454545454545455, 35: 3.545454545454545, 36: 3.636363636363636, 37: 3.727272727272727, 38: 3.818181818181818, 39: 3.909090909090909, 40: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'mindreading', 'fortunetelling', 'overgeneralizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 4 seconds\n        Selected: ['disqualifyingthepositive', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 5 seconds\n    Completed XGBoost feature selection in 5 seconds\nSelected 14 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'mindreading', 'fortunetelling', 'overgeneralizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative', 'personalizing', 'I_talk']\nTotal Time taken for featurewiz selection = 19 seconds\nOutput contains a list of 14 important features and a train dataframe\n    Time taken to create entire pipeline = 19 second(s)\nValidation Fold: 1\n{'best_params': {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(0)}, 'best_iter': np.int64(18), 'best_score': np.float64(0.5061364515802329)}\n[GPBoost] [Info] Total Bins 1646\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 14\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.212994\n[GPBoost] [Info] Start training from score 2.212994\nGPB_nrmse: 0.7998629758282726\nGPB_r: 0.7110660721640403\nGPB_params: {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(0)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -634.1882989674438 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1020.1298401831416 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1123.7456683720372 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1140.6919092485425 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1163.0123202235177 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6422379618642565\nmerf_r: 0.7695753377527315\n[GPBoost] [Info] Total Bins 1646\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 14\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.212994\n[GPBoost] [Info] Start training from score 2.212994\nSuper_nrmse: 0.6870860407623542\nSuper_r: 0.7412197665485812\nSuper_params: {'model__C': 3, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.272727272727273, 5: 1.3, 6: 1.363636363636364, 7: 1.4, 8: 1.409090909090909, 9: 1.454545454545455, 10: 1.545454545454545, 11: 1.6, 12: 1.636363636363636, 13: 1.666666666666667, 14: 1.7, 15: 1.727272727272727, 16: 1.818181818181818, 17: 1.909090909090909, 18: 1.954545454545455, 19: 2.0, 20: 2.090909090909091, 21: 2.181818181818182, 22: 2.272727272727273, 23: 2.363636363636364, 24: 2.454545454545455, 25: 2.545454545454545, 26: 2.636363636363636, 27: 2.666666666666667, 28: 2.7, 29: 2.727272727272727, 30: 2.772727272727273, 31: 2.818181818181818, 32: 2.909090909090909, 33: 3.0, 34: 3.090909090909091, 35: 3.181818181818182, 36: 3.272727272727273, 37: 3.363636363636364, 38: 3.454545454545455, 39: 3.545454545454545, 40: 3.636363636363636, 41: 3.727272727272727, 42: 3.818181818181818, 43: 3.909090909090909, 44: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (2) highly correlated variables:\n    ['I_talk', 'neutral']\n    Following (18) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\nCompleted SULOV. 18 features selected\nTime taken for SULOV method = 1 seconds\nFinally 18 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 2 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 5 seconds\n        Selected: ['personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 4 seconds\n        Selected: ['negative', 'words']\n            Time taken for regular XGBoost feature selection = 4 seconds\n    Completed XGBoost feature selection in 4 seconds\nSelected 15 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\nTotal Time taken for featurewiz selection = 18 seconds\nOutput contains a list of 15 important features and a train dataframe\n    Time taken to create entire pipeline = 18 second(s)\nValidation Fold: 2\n{'best_params': {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}, 'best_iter': np.int64(17), 'best_score': np.float64(0.5078384189398837)}\n[GPBoost] [Info] Total Bins 1693\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.211867\n[GPBoost] [Info] Start training from score 2.211867\nGPB_nrmse: 0.6740062564397812\nGPB_r: 0.7466007049097022\nGPB_params: {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -638.6810271923821 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1019.6917621470744 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1130.8008276841526 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1124.016005336088 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1123.9518563242918 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6294296621964024\nmerf_r: 0.780784897282632\n[GPBoost] [Info] Total Bins 1693\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.211867\n[GPBoost] [Info] Start training from score 2.211867\nSuper_nrmse: 0.6276488975732911\nSuper_r: 0.7820929464141791\nSuper_params: {'model__C': 2, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.272727272727273, 5: 1.3, 6: 1.363636363636364, 7: 1.4, 8: 1.409090909090909, 9: 1.454545454545455, 10: 1.545454545454545, 11: 1.6, 12: 1.636363636363636, 13: 1.666666666666667, 14: 1.7, 15: 1.727272727272727, 16: 1.818181818181818, 17: 1.909090909090909, 18: 1.954545454545455, 19: 2.0, 20: 2.090909090909091, 21: 2.1, 22: 2.181818181818182, 23: 2.272727272727273, 24: 2.363636363636364, 25: 2.454545454545455, 26: 2.545454545454545, 27: 2.575, 28: 2.636363636363636, 29: 2.666666666666667, 30: 2.7, 31: 2.727272727272727, 32: 2.818181818181818, 33: 2.909090909090909, 34: 3.0, 35: 3.090909090909091, 36: 3.181818181818182, 37: 3.272727272727273, 38: 3.363636363636364, 39: 3.454545454545455, 40: 3.545454545454545, 41: 3.636363636363636, 42: 3.727272727272727, 43: 3.818181818181818, 44: 3.909090909090909, 45: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 22 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 5 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 4 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 9 seconds\n        Selected: ['negative', 'positive', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 6 seconds\n    Completed XGBoost feature selection in 6 seconds\nSelected 15 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'I_talk', 'words']\nTotal Time taken for featurewiz selection = 47 seconds\nOutput contains a list of 15 important features and a train dataframe\n    Time taken to create entire pipeline = 46 second(s)\nValidation Fold: 3\n{'best_params': {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}, 'best_iter': np.int64(134), 'best_score': np.float64(0.47758942548796146)}\n[GPBoost] [Info] Total Bins 1761\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.168537\n[GPBoost] [Info] Start training from score 2.168537\nGPB_nrmse: 0.7451293558388883\nGPB_r: 0.6755254300036394\nGPB_params: {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -634.7632996855474 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1047.8027765972006 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1184.0868286651921 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1209.0914256872875 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1210.1072047780904 at iteration 5.\n</pre> <pre>merf_nrmse: 0.753786153592208\nmerf_r: 0.6590197437070664\n[GPBoost] [Info] Total Bins 1761\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.168537\n[GPBoost] [Info] Start training from score 2.168537\nSuper_nrmse: 0.8113736603939897\nSuper_r: 0.6027176112273506\nSuper_params: {'model__C': 10, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.272727272727273, 5: 1.3, 6: 1.363636363636364, 7: 1.4, 8: 1.409090909090909, 9: 1.454545454545455, 10: 1.545454545454545, 11: 1.6, 12: 1.636363636363636, 13: 1.666666666666667, 14: 1.7, 15: 1.727272727272727, 16: 1.818181818181818, 17: 1.909090909090909, 18: 1.954545454545455, 19: 2.0, 20: 2.090909090909091, 21: 2.1, 22: 2.181818181818182, 23: 2.272727272727273, 24: 2.363636363636364, 25: 2.454545454545455, 26: 2.545454545454545, 27: 2.575, 28: 2.636363636363636, 29: 2.666666666666667, 30: 2.727272727272727, 31: 2.772727272727273, 32: 2.818181818181818, 33: 2.909090909090909, 34: 3.0, 35: 3.090909090909091, 36: 3.181818181818182, 37: 3.272727272727273, 38: 3.363636363636364, 39: 3.454545454545455, 40: 3.545454545454545, 41: 3.636363636363636, 42: 3.727272727272727, 43: 3.818181818181818, 44: 3.909090909090909, 45: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 6 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 2 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['negative', 'positive', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 2 seconds\n    Completed XGBoost feature selection in 2 seconds\nSelected 15 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative', 'I_talk']\nTotal Time taken for featurewiz selection = 14 seconds\nOutput contains a list of 15 important features and a train dataframe\n    Time taken to create entire pipeline = 14 second(s)\nValidation Fold: 4\n{'best_params': {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}, 'best_iter': np.int64(85), 'best_score': np.float64(0.48374268122366815)}\n[GPBoost] [Info] Total Bins 1759\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.191502\n[GPBoost] [Info] Start training from score 2.191502\nGPB_nrmse: 0.6044468260333317\nGPB_r: 0.8034555182578965\nGPB_params: {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -632.4737110229543 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1031.04281777683 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1133.0666440040818 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1164.1484774320577 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1147.816053479364 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6529897301014177\nmerf_r: 0.7728284110112148\n[GPBoost] [Info] Total Bins 1759\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.191502\n[GPBoost] [Info] Start training from score 2.191502\nSuper_nrmse: 0.7166992071443611\nSuper_r: 0.6949055125426435\nSuper_params: {'model__C': 2, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nmerf mean r: 0.7618514290315255\nmerfmean nrmse: 0.6516432691838897\nsuper mean r: 0.7266100084979931\nsupermean nrmse: 0.6868663724847118\ngpb mean r: 0.7473286275647169\ngpbmean nrmse: 0.6861730124744726\n{'merf': np.float64(0.7618514290315255), 'super': np.float64(0.7266100084979931), 'gpb': np.float64(0.7473286275647169)}\n{'merf': np.float64(13.74382823394097), 'super': np.float64(9.895743295878711), 'gpb': np.float64(15.009613186749457)}\nTest fold: 4\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (381, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.4, 9: 1.409090909090909, 10: 1.454545454545455, 11: 1.545454545454545, 12: 1.6, 13: 1.636363636363636, 14: 1.666666666666667, 15: 1.727272727272727, 16: 1.818181818181818, 17: 1.909090909090909, 18: 2.0, 19: 2.090909090909091, 20: 2.1, 21: 2.181818181818182, 22: 2.272727272727273, 23: 2.363636363636364, 24: 2.4, 25: 2.454545454545455, 26: 2.545454545454545, 27: 2.575, 28: 2.636363636363636, 29: 2.666666666666667, 30: 2.7, 31: 2.727272727272727, 32: 2.772727272727273, 33: 2.818181818181818, 34: 2.909090909090909, 35: 3.0, 36: 3.090909090909091, 37: 3.181818181818182, 38: 3.272727272727273, 39: 3.363636363636364, 40: 3.454545454545455, 41: 3.545454545454545, 42: 3.636363636363636, 43: 3.727272727272727, 44: 3.818181818181818, 45: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 150 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 188 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 111 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 65 seconds\n        Selected: ['negative', 'positive', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 78 seconds\n    Completed XGBoost feature selection in 78 seconds\nSelected 15 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'I_talk', 'words']\nTotal Time taken for featurewiz selection = 591 seconds\nOutput contains a list of 15 important features and a train dataframe\n    Time taken to create entire pipeline = 591 second(s)\nValidation Fold: 0\n{'best_params': {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(5), 'lambda_l2': np.int64(0)}, 'best_iter': np.int64(11), 'best_score': np.float64(0.5060968532294529)}\n[GPBoost] [Info] Total Bins 1745\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.219778\n[GPBoost] [Info] Start training from score 2.219778\nGPB_nrmse: 0.7926579856178125\nGPB_r: 0.7043836865098322\nGPB_params: {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(5), 'lambda_l2': np.int64(0)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -633.0992472485644 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1035.2249766722778 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1159.9587169663034 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1180.4037245809916 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1191.7816363440925 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6776819921845018\nmerf_r: 0.7641865130022685\n[GPBoost] [Info] Total Bins 1745\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.219778\n[GPBoost] [Info] Start training from score 2.219778\nSuper_nrmse: 0.7111626459623392\nSuper_r: 0.7452354564288909\nSuper_params: {'model__C': 5, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (381, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.181818181818182, 3: 1.2, 4: 1.272727272727273, 5: 1.3, 6: 1.363636363636364, 7: 1.4, 8: 1.409090909090909, 9: 1.454545454545455, 10: 1.545454545454545, 11: 1.6, 12: 1.636363636363636, 13: 1.727272727272727, 14: 1.818181818181818, 15: 1.909090909090909, 16: 1.954545454545455, 17: 2.0, 18: 2.090909090909091, 19: 2.1, 20: 2.181818181818182, 21: 2.272727272727273, 22: 2.363636363636364, 23: 2.454545454545455, 24: 2.545454545454545, 25: 2.636363636363636, 26: 2.666666666666667, 27: 2.7, 28: 2.727272727272727, 29: 2.772727272727273, 30: 2.818181818181818, 31: 2.909090909090909, 32: 3.0, 33: 3.090909090909091, 34: 3.181818181818182, 35: 3.272727272727273, 36: 3.363636363636364, 37: 3.454545454545455, 38: 3.545454545454545, 39: 3.636363636363636, 40: 3.727272727272727, 41: 3.818181818181818, 42: 3.909090909090909, 43: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 1 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 288 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 121 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 24 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 10 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 11 seconds\n    Completed XGBoost feature selection in 11 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative', 'I_talk']\nTotal Time taken for featurewiz selection = 454 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 454 second(s)\nValidation Fold: 1\n{'best_params': {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}, 'best_iter': np.int64(17), 'best_score': np.float64(0.5151821219201549)}\n[GPBoost] [Info] Total Bins 1820\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.176195\n[GPBoost] [Info] Start training from score 2.176195\nGPB_nrmse: 0.6915524451542395\nGPB_r: 0.7323389750345182\nGPB_params: {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -636.3251952151942 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1015.3497667690161 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1112.7157041536873 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1129.9842564936641 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1146.0927093941357 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6255437390938694\nmerf_r: 0.7788597022570779\n[GPBoost] [Info] Total Bins 1820\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.176195\n[GPBoost] [Info] Start training from score 2.176195\nSuper_nrmse: 0.6296831443605467\nSuper_r: 0.7749833904800089\nSuper_params: {'model__C': 5, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.272727272727273, 5: 1.3, 6: 1.363636363636364, 7: 1.4, 8: 1.409090909090909, 9: 1.454545454545455, 10: 1.545454545454545, 11: 1.6, 12: 1.636363636363636, 13: 1.666666666666667, 14: 1.727272727272727, 15: 1.818181818181818, 16: 1.909090909090909, 17: 1.954545454545455, 18: 2.0, 19: 2.090909090909091, 20: 2.1, 21: 2.181818181818182, 22: 2.272727272727273, 23: 2.363636363636364, 24: 2.4, 25: 2.454545454545455, 26: 2.545454545454545, 27: 2.575, 28: 2.636363636363636, 29: 2.7, 30: 2.727272727272727, 31: 2.772727272727273, 32: 2.818181818181818, 33: 2.909090909090909, 34: 3.0, 35: 3.090909090909091, 36: 3.181818181818182, 37: 3.272727272727273, 38: 3.363636363636364, 39: 3.454545454545455, 40: 3.545454545454545, 41: 3.636363636363636, 42: 3.727272727272727, 43: 3.818181818181818, 44: 3.909090909090909, 45: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (2) highly correlated variables:\n    ['I_talk', 'neutral']\n    Following (18) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\nCompleted SULOV. 18 features selected\nTime taken for SULOV method = 0 seconds\nFinally 18 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'dichotomousreasoning', 'minimization', 'mindreading', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 2 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 8 seconds\n        Selected: ['personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 2 seconds\n        Selected: ['negative', 'words']\n            Time taken for regular XGBoost feature selection = 8 seconds\n    Completed XGBoost feature selection in 8 seconds\nSelected 12 important features:\n['comparison', 'dichotomousreasoning', 'minimization', 'mindreading', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words', 'fortunetelling']\nTotal Time taken for featurewiz selection = 23 seconds\nOutput contains a list of 12 important features and a train dataframe\n    Time taken to create entire pipeline = 23 second(s)\nValidation Fold: 2\n{'best_params': {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(0)}, 'best_iter': np.int64(103), 'best_score': np.float64(0.4935404172723145)}\n[GPBoost] [Info] Total Bins 1436\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 12\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.204607\n[GPBoost] [Info] Start training from score 2.204607\nGPB_nrmse: 0.7215448393093222\nGPB_r: 0.7051956263898058\nGPB_params: {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(0)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -637.4594116812412 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1033.4900421442505 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1144.6324044852117 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1168.4198666809173 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1169.0429943515117 at iteration 5.\n</pre> <pre>merf_nrmse: 0.7040436149447202\nmerf_r: 0.7087303464428599\n[GPBoost] [Info] Total Bins 1436\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 12\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.204607\n[GPBoost] [Info] Start training from score 2.204607\nSuper_nrmse: 0.7420385120685433\nSuper_r: 0.676236073816104\nSuper_params: {'model__C': 10, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.4, 9: 1.454545454545455, 10: 1.545454545454545, 11: 1.6, 12: 1.636363636363636, 13: 1.666666666666667, 14: 1.727272727272727, 15: 1.818181818181818, 16: 1.909090909090909, 17: 1.954545454545455, 18: 2.0, 19: 2.090909090909091, 20: 2.181818181818182, 21: 2.272727272727273, 22: 2.363636363636364, 23: 2.4, 24: 2.454545454545455, 25: 2.545454545454545, 26: 2.575, 27: 2.636363636363636, 28: 2.666666666666667, 29: 2.727272727272727, 30: 2.818181818181818, 31: 2.909090909090909, 32: 3.0, 33: 3.090909090909091, 34: 3.181818181818182, 35: 3.272727272727273, 36: 3.363636363636364, 37: 3.454545454545455, 38: 3.545454545454545, 39: 3.636363636363636, 40: 3.727272727272727, 41: 3.818181818181818, 42: 3.909090909090909, 43: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 5 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 4 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 5 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 4 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 4 seconds\n    Completed XGBoost feature selection in 4 seconds\nSelected 15 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative', 'fortunetelling', 'I_talk']\nTotal Time taken for featurewiz selection = 20 seconds\nOutput contains a list of 15 important features and a train dataframe\n    Time taken to create entire pipeline = 20 second(s)\nValidation Fold: 3\n{'best_params': {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}, 'best_iter': np.int64(102), 'best_score': np.float64(0.4945621239483577)}\n[GPBoost] [Info] Total Bins 1754\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.163611\n[GPBoost] [Info] Start training from score 2.163611\nGPB_nrmse: 0.5584817731578646\nGPB_r: 0.8323688316478413\nGPB_params: {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -627.6184409198349 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1015.3651641082317 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1129.4947700892556 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1162.1563391595912 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1155.6364025252776 at iteration 5.\n</pre> <pre>merf_nrmse: 0.5837317173744324\nmerf_r: 0.8198515843575707\n[GPBoost] [Info] Total Bins 1754\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.163611\n[GPBoost] [Info] Start training from score 2.163611\nSuper_nrmse: 0.6506088154736884\nSuper_r: 0.7615536951615\nSuper_params: {'model__C': 6, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.409090909090909, 9: 1.454545454545455, 10: 1.545454545454545, 11: 1.636363636363636, 12: 1.666666666666667, 13: 1.727272727272727, 14: 1.818181818181818, 15: 1.909090909090909, 16: 1.954545454545455, 17: 2.0, 18: 2.090909090909091, 19: 2.1, 20: 2.181818181818182, 21: 2.272727272727273, 22: 2.363636363636364, 23: 2.4, 24: 2.454545454545455, 25: 2.545454545454545, 26: 2.575, 27: 2.636363636363636, 28: 2.666666666666667, 29: 2.7, 30: 2.727272727272727, 31: 2.772727272727273, 32: 2.818181818181818, 33: 2.909090909090909, 34: 3.0, 35: 3.090909090909091, 36: 3.181818181818182, 37: 3.272727272727273, 38: 3.363636363636364, 39: 3.454545454545455, 40: 3.545454545454545, 41: 3.636363636363636, 42: 3.727272727272727, 43: 3.818181818181818, 44: 3.909090909090909, 45: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 4 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 3 seconds\n    Completed XGBoost feature selection in 3 seconds\nSelected 15 important features:\n['labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative', 'I_talk', 'mentalfiltering']\nTotal Time taken for featurewiz selection = 16 seconds\nOutput contains a list of 15 important features and a train dataframe\n    Time taken to create entire pipeline = 15 second(s)\nValidation Fold: 4\n{'best_params': {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(1)}, 'best_iter': np.int64(8), 'best_score': np.float64(0.5065901979692852)}\n[GPBoost] [Info] Total Bins 1707\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.191240\n[GPBoost] [Info] Start training from score 2.191240\nGPB_nrmse: 0.8169010347298862\nGPB_r: 0.6182423257293443\nGPB_params: {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(1)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -635.5939887313465 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1031.9326896823975 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1151.5362823584005 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1176.0141512015045 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1191.9369699866354 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6676814445812376\nmerf_r: 0.7488547263453293\n[GPBoost] [Info] Total Bins 1707\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.191240\n[GPBoost] [Info] Start training from score 2.191240\nSuper_nrmse: 0.6806268776703185\nSuper_r: 0.7293944560548974\nSuper_params: {'model__C': 5, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nmerf mean r: 0.7640965744810212\nmerfmean nrmse: 0.6517365016357523\nsuper mean r: 0.7374806143882803\nsupermean nrmse: 0.6828239991070872\ngpb mean r: 0.7185058890622683\ngpbmean nrmse: 0.716227615593825\n{'merf': np.float64(0.7640965744810212), 'super': np.float64(0.7374806143882803), 'gpb': np.float64(0.7185058890622683)}\n{'merf': np.float64(20.99243685370252), 'super': np.float64(21.537649507530965), 'gpb': np.float64(10.46147055883768)}\nTest fold: 5\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (381, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.4, 9: 1.409090909090909, 10: 1.454545454545455, 11: 1.545454545454545, 12: 1.6, 13: 1.636363636363636, 14: 1.666666666666667, 15: 1.7, 16: 1.727272727272727, 17: 1.818181818181818, 18: 1.909090909090909, 19: 2.0, 20: 2.090909090909091, 21: 2.1, 22: 2.181818181818182, 23: 2.272727272727273, 24: 2.363636363636364, 25: 2.4, 26: 2.454545454545455, 27: 2.545454545454545, 28: 2.575, 29: 2.636363636363636, 30: 2.666666666666667, 31: 2.7, 32: 2.727272727272727, 33: 2.772727272727273, 34: 2.818181818181818, 35: 2.909090909090909, 36: 3.0, 37: 3.090909090909091, 38: 3.181818181818182, 39: 3.272727272727273, 40: 3.363636363636364, 41: 3.454545454545455, 42: 3.545454545454545, 43: 3.636363636363636, 44: 3.727272727272727, 45: 3.818181818181818, 46: 3.909090909090909, 47: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 4 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 4 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 4 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 4 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 4 seconds\n    Completed XGBoost feature selection in 4 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nTotal Time taken for featurewiz selection = 21 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 20 second(s)\nValidation Fold: 0\n{'best_params': {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}, 'best_iter': np.int64(14), 'best_score': np.float64(0.5174606981254491)}\n[GPBoost] [Info] Total Bins 1825\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.184393\n[GPBoost] [Info] Start training from score 2.184393\nGPB_nrmse: 0.748534606399539\nGPB_r: 0.7287501575918875\nGPB_params: {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -634.804372044795 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1025.505350831523 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1152.4547584490372 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1173.2815032361893 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1174.2463514007507 at iteration 5.\n</pre> <pre>merf_nrmse: 0.5717837432923563\nmerf_r: 0.8227044120980458\n[GPBoost] [Info] Total Bins 1825\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.184393\n[GPBoost] [Info] Start training from score 2.184393\nSuper_nrmse: 0.6081781566506729\nSuper_r: 0.8042720932876547\nSuper_params: {'model__C': 5, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (381, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.181818181818182, 3: 1.2, 4: 1.272727272727273, 5: 1.3, 6: 1.363636363636364, 7: 1.4, 8: 1.454545454545455, 9: 1.545454545454545, 10: 1.6, 11: 1.636363636363636, 12: 1.7, 13: 1.727272727272727, 14: 1.818181818181818, 15: 1.909090909090909, 16: 1.954545454545455, 17: 2.0, 18: 2.090909090909091, 19: 2.181818181818182, 20: 2.272727272727273, 21: 2.363636363636364, 22: 2.454545454545455, 23: 2.545454545454545, 24: 2.636363636363636, 25: 2.666666666666667, 26: 2.7, 27: 2.727272727272727, 28: 2.818181818181818, 29: 2.909090909090909, 30: 3.0, 31: 3.090909090909091, 32: 3.181818181818182, 33: 3.272727272727273, 34: 3.363636363636364, 35: 3.454545454545455, 36: 3.545454545454545, 37: 3.636363636363636, 38: 3.727272727272727, 39: 3.818181818181818, 40: 3.909090909090909, 41: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (2) highly correlated variables:\n    ['I_talk', 'neutral']\n    Following (18) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\nCompleted SULOV. 18 features selected\nTime taken for SULOV method = 0 seconds\nFinally 18 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 4 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 4 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 4 seconds\n        Selected: ['personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 4 seconds\n        Selected: ['negative', 'words']\n            Time taken for regular XGBoost feature selection = 4 seconds\n    Completed XGBoost feature selection in 4 seconds\nSelected 15 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words', 'minimization', 'mentalfiltering']\nTotal Time taken for featurewiz selection = 20 seconds\nOutput contains a list of 15 important features and a train dataframe\n    Time taken to create entire pipeline = 19 second(s)\nValidation Fold: 1\n{'best_params': {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(1)}, 'best_iter': np.int64(5), 'best_score': np.float64(0.5138358886018839)}\n[GPBoost] [Info] Total Bins 1695\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.203301\n[GPBoost] [Info] Start training from score 2.203301\nGPB_nrmse: 0.7224156880221276\nGPB_r: 0.7263805539920537\nGPB_params: {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(1)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -628.437616089423 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1011.6085028448146 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1119.4778170196064 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1136.9982429469737 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1137.4397320623877 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6255594221642095\nmerf_r: 0.7838866196272769\n[GPBoost] [Info] Total Bins 1695\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.203301\n[GPBoost] [Info] Start training from score 2.203301\nSuper_nrmse: 0.6111058579244746\nSuper_r: 0.7900194673155759\nSuper_params: {'model__C': 10, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'scale', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.272727272727273, 5: 1.3, 6: 1.363636363636364, 7: 1.4, 8: 1.409090909090909, 9: 1.454545454545455, 10: 1.545454545454545, 11: 1.6, 12: 1.636363636363636, 13: 1.666666666666667, 14: 1.7, 15: 1.727272727272727, 16: 1.818181818181818, 17: 1.909090909090909, 18: 1.954545454545455, 19: 2.0, 20: 2.090909090909091, 21: 2.1, 22: 2.181818181818182, 23: 2.272727272727273, 24: 2.363636363636364, 25: 2.4, 26: 2.454545454545455, 27: 2.545454545454545, 28: 2.575, 29: 2.636363636363636, 30: 2.666666666666667, 31: 2.7, 32: 2.727272727272727, 33: 2.772727272727273, 34: 2.818181818181818, 35: 2.909090909090909, 36: 3.0, 37: 3.090909090909091, 38: 3.181818181818182, 39: 3.272727272727273, 40: 3.363636363636364, 41: 3.454545454545455, 42: 3.545454545454545, 43: 3.636363636363636, 44: 3.727272727272727, 45: 3.818181818181818, 46: 3.909090909090909, 47: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 4 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 4 seconds\n        Selected: ['negative', 'positive', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 4 seconds\n    Completed XGBoost feature selection in 4 seconds\nSelected 15 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'positive', 'srs_ges', 'negative', 'words']\nTotal Time taken for featurewiz selection = 18 seconds\nOutput contains a list of 15 important features and a train dataframe\n    Time taken to create entire pipeline = 18 second(s)\nValidation Fold: 2\n{'best_params': {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}, 'best_iter': np.int64(14), 'best_score': np.float64(0.4804956276243891)}\n[GPBoost] [Info] Total Bins 1753\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.154710\n[GPBoost] [Info] Start training from score 2.154710\nGPB_nrmse: 0.8033937032989119\nGPB_r: 0.6637812136433185\nGPB_params: {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -639.2516645952868 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1030.0377057435794 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1143.4638090119254 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1161.241986408624 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1160.8159801728339 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6671271031311\nmerf_r: 0.7539859973382709\n[GPBoost] [Info] Total Bins 1753\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.154710\n[GPBoost] [Info] Start training from score 2.154710\nSuper_nrmse: 0.6821140441274113\nSuper_r: 0.7461726870578896\nSuper_params: {'model__C': 10, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.4, 9: 1.409090909090909, 10: 1.454545454545455, 11: 1.545454545454545, 12: 1.6, 13: 1.636363636363636, 14: 1.666666666666667, 15: 1.727272727272727, 16: 1.818181818181818, 17: 1.909090909090909, 18: 1.954545454545455, 19: 2.0, 20: 2.090909090909091, 21: 2.1, 22: 2.181818181818182, 23: 2.272727272727273, 24: 2.363636363636364, 25: 2.4, 26: 2.454545454545455, 27: 2.545454545454545, 28: 2.575, 29: 2.636363636363636, 30: 2.727272727272727, 31: 2.772727272727273, 32: 2.818181818181818, 33: 2.909090909090909, 34: 3.0, 35: 3.090909090909091, 36: 3.181818181818182, 37: 3.272727272727273, 38: 3.363636363636364, 39: 3.454545454545455, 40: 3.545454545454545, 41: 3.636363636363636, 42: 3.727272727272727, 43: 3.818181818181818, 44: 3.909090909090909, 45: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 4 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 4 seconds\n    Completed XGBoost feature selection in 4 seconds\nSelected 15 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative', 'I_talk']\nTotal Time taken for featurewiz selection = 17 seconds\nOutput contains a list of 15 important features and a train dataframe\n    Time taken to create entire pipeline = 17 second(s)\nValidation Fold: 3\n{'best_params': {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(1)}, 'best_iter': np.int64(60), 'best_score': np.float64(0.49520413969340016)}\n[GPBoost] [Info] Total Bins 1747\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.174264\n[GPBoost] [Info] Start training from score 2.174264\nGPB_nrmse: 0.6165506980261425\nGPB_r: 0.7847540944454262\nGPB_params: {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(1)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -634.3354549645551 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1031.4442749899238 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1138.5809116560079 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1156.868896080112 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1151.3015863054447 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6319479472439188\nmerf_r: 0.7784173493002581\n[GPBoost] [Info] Total Bins 1747\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.174264\n[GPBoost] [Info] Start training from score 2.174264\nSuper_nrmse: 0.6906958596875381\nSuper_r: 0.7214643672353706\nSuper_params: {'model__C': 9, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.409090909090909, 9: 1.454545454545455, 10: 1.545454545454545, 11: 1.636363636363636, 12: 1.666666666666667, 13: 1.7, 14: 1.727272727272727, 15: 1.818181818181818, 16: 1.909090909090909, 17: 1.954545454545455, 18: 2.0, 19: 2.090909090909091, 20: 2.1, 21: 2.181818181818182, 22: 2.272727272727273, 23: 2.363636363636364, 24: 2.4, 25: 2.454545454545455, 26: 2.545454545454545, 27: 2.575, 28: 2.636363636363636, 29: 2.666666666666667, 30: 2.7, 31: 2.727272727272727, 32: 2.772727272727273, 33: 2.818181818181818, 34: 2.909090909090909, 35: 3.0, 36: 3.090909090909091, 37: 3.181818181818182, 38: 3.272727272727273, 39: 3.363636363636364, 40: 3.454545454545455, 41: 3.545454545454545, 42: 3.636363636363636, 43: 3.727272727272727, 44: 3.818181818181818, 45: 3.909090909090909, 46: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 4 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 3 seconds\n        Selected: ['negative', 'positive', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 4 seconds\n    Completed XGBoost feature selection in 4 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nTotal Time taken for featurewiz selection = 18 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 18 second(s)\nValidation Fold: 4\n{'best_params': {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(0)}, 'best_iter': np.int64(9), 'best_score': np.float64(0.48664735992629093)}\n[GPBoost] [Info] Total Bins 1823\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.191811\n[GPBoost] [Info] Start training from score 2.191811\nGPB_nrmse: 0.7640521249240323\nGPB_r: 0.6814867464253939\nGPB_params: {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(0)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -647.0686020867427 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1045.9371122510152 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1161.6154200746212 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1190.6564189718915 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1184.3259423321788 at iteration 5.\n</pre> <pre>merf_nrmse: 0.716411592693441\nmerf_r: 0.6981397275518346\n[GPBoost] [Info] Total Bins 1823\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.191811\n[GPBoost] [Info] Start training from score 2.191811\nSuper_nrmse: 0.7031415849901173\nSuper_r: 0.7094330481672332\nSuper_params: {'model__C': 8, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'scale', 'model__kernel': 'rbf'}\nmerf mean r: 0.7674268211831372\nmerfmean nrmse: 0.642565961705005\nsuper mean r: 0.7542723326127448\nsupermean nrmse: 0.6590471006760428\ngpb mean r: 0.717030553219616\ngpbmean nrmse: 0.7309893641341507\n{'merf': np.float64(0.7674268211831372), 'super': np.float64(0.7542723326127448), 'gpb': np.float64(0.717030553219616)}\n{'merf': np.float64(18.691409089816307), 'super': np.float64(20.259803098839125), 'gpb': np.float64(16.985615005201645)}\nTest fold: 6\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (381, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.4, 9: 1.409090909090909, 10: 1.454545454545455, 11: 1.545454545454545, 12: 1.636363636363636, 13: 1.666666666666667, 14: 1.7, 15: 1.727272727272727, 16: 1.818181818181818, 17: 1.909090909090909, 18: 2.0, 19: 2.090909090909091, 20: 2.1, 21: 2.181818181818182, 22: 2.272727272727273, 23: 2.363636363636364, 24: 2.454545454545455, 25: 2.545454545454545, 26: 2.575, 27: 2.636363636363636, 28: 2.7, 29: 2.727272727272727, 30: 2.818181818181818, 31: 2.909090909090909, 32: 3.0, 33: 3.090909090909091, 34: 3.181818181818182, 35: 3.272727272727273, 36: 3.363636363636364, 37: 3.454545454545455, 38: 3.545454545454545, 39: 3.636363636363636, 40: 3.727272727272727, 41: 3.818181818181818, 42: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['negative', 'positive', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 15 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'positive', 'srs_ges', 'negative', 'words']\nTotal Time taken for featurewiz selection = 3 seconds\nOutput contains a list of 15 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\nValidation Fold: 0\n{'best_params': {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}, 'best_iter': np.int64(38), 'best_score': np.float64(0.5201563313687345)}\n[GPBoost] [Info] Total Bins 1755\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.173385\n[GPBoost] [Info] Start training from score 2.173385\nGPB_nrmse: 0.5318033905111615\nGPB_r: 0.8469976571195883\nGPB_params: {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -631.4832693602684 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1032.5595662786177 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1145.444640743609 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1182.5010167939802 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1168.2020457324263 at iteration 5.\n</pre> <pre>merf_nrmse: 0.5393208006444491\nmerf_r: 0.8460019522269782\n[GPBoost] [Info] Total Bins 1755\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.173385\n[GPBoost] [Info] Start training from score 2.173385\nSuper_nrmse: 0.5785610140071107\nSuper_r: 0.815132398700036\nSuper_params: {'model__C': 5, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (381, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.272727272727273, 5: 1.3, 6: 1.363636363636364, 7: 1.4, 8: 1.454545454545455, 9: 1.545454545454545, 10: 1.6, 11: 1.636363636363636, 12: 1.666666666666667, 13: 1.727272727272727, 14: 1.818181818181818, 15: 1.909090909090909, 16: 1.954545454545455, 17: 2.0, 18: 2.090909090909091, 19: 2.1, 20: 2.181818181818182, 21: 2.272727272727273, 22: 2.363636363636364, 23: 2.4, 24: 2.454545454545455, 25: 2.545454545454545, 26: 2.575, 27: 2.636363636363636, 28: 2.727272727272727, 29: 2.818181818181818, 30: 2.909090909090909, 31: 3.0, 32: 3.090909090909091, 33: 3.181818181818182, 34: 3.272727272727273, 35: 3.363636363636364, 36: 3.454545454545455, 37: 3.545454545454545, 38: 3.636363636363636, 39: 3.727272727272727, 40: 3.818181818181818, 41: 3.909090909090909, 42: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'disqualifyingthepositive', 'mentalfiltering', 'I_talk', 'words']\nTotal Time taken for featurewiz selection = 3 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\nValidation Fold: 1\n{'best_params': {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}, 'best_iter': np.int64(91), 'best_score': np.float64(0.5170747718248968)}\n[GPBoost] [Info] Total Bins 1823\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.174197\n[GPBoost] [Info] Start training from score 2.174197\nGPB_nrmse: 0.5750291414901605\nGPB_r: 0.827309133719287\nGPB_params: {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -633.4345012847367 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1025.1482791714616 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1133.3541189997482 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1172.9796642980675 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1173.6352675432636 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6113594369708445\nmerf_r: 0.8017414404443679\n[GPBoost] [Info] Total Bins 1823\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.174197\n[GPBoost] [Info] Start training from score 2.174197\nSuper_nrmse: 0.6646148745498567\nSuper_r: 0.7513000756717853\nSuper_params: {'model__C': 7, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.181818181818182, 3: 1.2, 4: 1.272727272727273, 5: 1.3, 6: 1.363636363636364, 7: 1.4, 8: 1.409090909090909, 9: 1.454545454545455, 10: 1.545454545454545, 11: 1.6, 12: 1.636363636363636, 13: 1.666666666666667, 14: 1.7, 15: 1.727272727272727, 16: 1.818181818181818, 17: 1.909090909090909, 18: 1.954545454545455, 19: 2.0, 20: 2.090909090909091, 21: 2.181818181818182, 22: 2.272727272727273, 23: 2.363636363636364, 24: 2.4, 25: 2.454545454545455, 26: 2.545454545454545, 27: 2.575, 28: 2.636363636363636, 29: 2.7, 30: 2.727272727272727, 31: 2.818181818181818, 32: 2.909090909090909, 33: 3.0, 34: 3.090909090909091, 35: 3.181818181818182, 36: 3.272727272727273, 37: 3.363636363636364, 38: 3.454545454545455, 39: 3.545454545454545, 40: 3.636363636363636, 41: 3.727272727272727, 42: 3.818181818181818, 43: 3.909090909090909, 44: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 14 important features:\n['comparison', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative', 'I_talk']\nTotal Time taken for featurewiz selection = 3 seconds\nOutput contains a list of 14 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\nValidation Fold: 2\n{'best_params': {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}, 'best_iter': np.int64(115), 'best_score': np.float64(0.4638554958903652)}\n[GPBoost] [Info] Total Bins 1653\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 14\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.209962\n[GPBoost] [Info] Start training from score 2.209962\nGPB_nrmse: 0.7235916482741503\nGPB_r: 0.7078372886693971\nGPB_params: {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -641.052590407575 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1036.1223325271367 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1166.2779447707007 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1208.4576970396347 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1213.0818644274593 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6883463546726475\nmerf_r: 0.7287776441399089\n[GPBoost] [Info] Total Bins 1653\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 14\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.209962\n[GPBoost] [Info] Start training from score 2.209962\nSuper_nrmse: 0.7144143245845334\nSuper_r: 0.7028272856131367\nSuper_params: {'model__C': 10, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.4, 9: 1.409090909090909, 10: 1.454545454545455, 11: 1.545454545454545, 12: 1.6, 13: 1.636363636363636, 14: 1.666666666666667, 15: 1.7, 16: 1.727272727272727, 17: 1.818181818181818, 18: 1.909090909090909, 19: 1.954545454545455, 20: 2.0, 21: 2.090909090909091, 22: 2.1, 23: 2.181818181818182, 24: 2.272727272727273, 25: 2.363636363636364, 26: 2.4, 27: 2.454545454545455, 28: 2.545454545454545, 29: 2.636363636363636, 30: 2.7, 31: 2.727272727272727, 32: 2.818181818181818, 33: 2.909090909090909, 34: 3.0, 35: 3.090909090909091, 36: 3.181818181818182, 37: 3.272727272727273, 38: 3.363636363636364, 39: 3.454545454545455, 40: 3.545454545454545, 41: 3.636363636363636, 42: 3.727272727272727, 43: 3.818181818181818, 44: 3.909090909090909, 45: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (2) highly correlated variables:\n    ['I_talk', 'neutral']\n    Following (18) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\nCompleted SULOV. 18 features selected\nTime taken for SULOV method = 0 seconds\nFinally 18 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 15 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\nTotal Time taken for featurewiz selection = 3 seconds\nOutput contains a list of 15 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\nValidation Fold: 3\n{'best_params': {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(5), 'lambda_l2': np.int64(10)}, 'best_iter': np.int64(24), 'best_score': np.float64(0.49131470140294314)}\n[GPBoost] [Info] Total Bins 1701\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.172331\n[GPBoost] [Info] Start training from score 2.172331\nGPB_nrmse: 0.7788979487371724\nGPB_r: 0.663677431861305\nGPB_params: {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(5), 'lambda_l2': np.int64(10)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -640.1737097376314 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1038.0692798264658 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1165.0640155253072 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1223.1486000160933 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1217.8411733466817 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6896869227636814\nmerf_r: 0.7248882807431088\n[GPBoost] [Info] Total Bins 1701\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.172331\n[GPBoost] [Info] Start training from score 2.172331\nSuper_nrmse: 0.7274684322335876\nSuper_r: 0.6945041901859201\nSuper_params: {'model__C': 5, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.409090909090909, 9: 1.454545454545455, 10: 1.545454545454545, 11: 1.6, 12: 1.636363636363636, 13: 1.7, 14: 1.727272727272727, 15: 1.818181818181818, 16: 1.909090909090909, 17: 1.954545454545455, 18: 2.0, 19: 2.090909090909091, 20: 2.1, 21: 2.181818181818182, 22: 2.272727272727273, 23: 2.363636363636364, 24: 2.4, 25: 2.454545454545455, 26: 2.545454545454545, 27: 2.575, 28: 2.636363636363636, 29: 2.7, 30: 2.727272727272727, 31: 2.818181818181818, 32: 2.909090909090909, 33: 3.0, 34: 3.090909090909091, 35: 3.181818181818182, 36: 3.272727272727273, 37: 3.363636363636364, 38: 3.454545454545455, 39: 3.545454545454545, 40: 3.636363636363636, 41: 3.727272727272727, 42: 3.818181818181818, 43: 3.909090909090909, 44: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['negative', 'positive', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 15 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nTotal Time taken for featurewiz selection = 3 seconds\nOutput contains a list of 15 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\nValidation Fold: 4\n{'best_params': {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}, 'best_iter': np.int64(11), 'best_score': np.float64(0.5201560923734034)}\n[GPBoost] [Info] Total Bins 1753\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.207217\n[GPBoost] [Info] Start training from score 2.207217\nGPB_nrmse: 0.7907813461018247\nGPB_r: 0.6396344323980041\nGPB_params: {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -635.1976381375501 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1023.8682365202287 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1133.0816695516971 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1152.863648078605 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1167.2256881341664 at iteration 5.\n</pre> <pre>merf_nrmse: 0.643261614122246\nmerf_r: 0.7735839717282665\n[GPBoost] [Info] Total Bins 1753\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.207217\n[GPBoost] [Info] Start training from score 2.207217\nSuper_nrmse: 0.6714409175213771\nSuper_r: 0.7400603983482411\nSuper_params: {'model__C': 9, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nmerf mean r: 0.774998657856526\nmerfmean nrmse: 0.6343950258347737\nsuper mean r: 0.7407648697038238\nsupermean nrmse: 0.6712999125792931\ngpb mean r: 0.7370911887535162\ngpbmean nrmse: 0.6800206950228939\n{'merf': np.float64(0.774998657856526), 'super': np.float64(0.7407648697038238), 'gpb': np.float64(0.7370911887535162)}\n{'merf': np.float64(16.988006678365345), 'super': np.float64(17.249188549055898), 'gpb': np.float64(8.691303438035678)}\nTest fold: 7\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (381, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.454545454545455, 9: 1.545454545454545, 10: 1.6, 11: 1.636363636363636, 12: 1.7, 13: 1.727272727272727, 14: 1.818181818181818, 15: 1.909090909090909, 16: 1.954545454545455, 17: 2.0, 18: 2.090909090909091, 19: 2.1, 20: 2.181818181818182, 21: 2.272727272727273, 22: 2.363636363636364, 23: 2.4, 24: 2.454545454545455, 25: 2.545454545454545, 26: 2.575, 27: 2.636363636363636, 28: 2.666666666666667, 29: 2.7, 30: 2.727272727272727, 31: 2.772727272727273, 32: 2.818181818181818, 33: 2.909090909090909, 34: 3.0, 35: 3.090909090909091, 36: 3.181818181818182, 37: 3.272727272727273, 38: 3.363636363636364, 39: 3.454545454545455, 40: 3.545454545454545, 41: 3.636363636363636, 42: 3.727272727272727, 43: 3.818181818181818, 44: 3.909090909090909, 45: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (2) highly correlated variables:\n    ['I_talk', 'neutral']\n    Following (18) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\nCompleted SULOV. 18 features selected\nTime taken for SULOV method = 0 seconds\nFinally 18 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 14 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\nTotal Time taken for featurewiz selection = 3 seconds\nOutput contains a list of 14 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\nValidation Fold: 0\n{'best_params': {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(0)}, 'best_iter': np.int64(9), 'best_score': np.float64(0.5065191653492256)}\n[GPBoost] [Info] Total Bins 1621\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 14\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.163865\n[GPBoost] [Info] Start training from score 2.163865\nGPB_nrmse: 0.7454651761894024\nGPB_r: 0.7014146747320423\nGPB_params: {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(0)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -631.9403641458475 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1011.2166650930897 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1118.0153295325542 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1140.2368720920952 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1137.38449356231 at iteration 5.\n</pre> <pre>merf_nrmse: 0.622728172007396\nmerf_r: 0.7889947952793666\n[GPBoost] [Info] Total Bins 1621\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 14\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.163865\n[GPBoost] [Info] Start training from score 2.163865\nSuper_nrmse: 0.6488301676565871\nSuper_r: 0.7715923795501846\nSuper_params: {'model__C': 10, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'scale', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (381, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.4, 9: 1.409090909090909, 10: 1.454545454545455, 11: 1.545454545454545, 12: 1.636363636363636, 13: 1.666666666666667, 14: 1.727272727272727, 15: 1.818181818181818, 16: 1.909090909090909, 17: 1.954545454545455, 18: 2.0, 19: 2.090909090909091, 20: 2.1, 21: 2.181818181818182, 22: 2.272727272727273, 23: 2.363636363636364, 24: 2.4, 25: 2.454545454545455, 26: 2.545454545454545, 27: 2.575, 28: 2.636363636363636, 29: 2.666666666666667, 30: 2.727272727272727, 31: 2.772727272727273, 32: 2.818181818181818, 33: 2.909090909090909, 34: 3.0, 35: 3.090909090909091, 36: 3.181818181818182, 37: 3.272727272727273, 38: 3.363636363636364, 39: 3.454545454545455, 40: 3.545454545454545, 41: 3.636363636363636, 42: 3.727272727272727, 43: 3.818181818181818, 44: 3.909090909090909, 45: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 15 important features:\n['labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative', 'fortunetelling', 'I_talk']\nTotal Time taken for featurewiz selection = 3 seconds\nOutput contains a list of 15 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\nValidation Fold: 1\n{'best_params': {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(1)}, 'best_iter': np.int64(67), 'best_score': np.float64(0.5096276197631812)}\n[GPBoost] [Info] Total Bins 1703\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.192076\n[GPBoost] [Info] Start training from score 2.192076\nGPB_nrmse: 0.678559717019812\nGPB_r: 0.7344377711017896\nGPB_params: {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(1)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -643.4027972569144 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1038.1401087283589 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1142.516277063222 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1163.9095582642258 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1166.5426767585936 at iteration 5.\n</pre> <pre>merf_nrmse: 0.7508835719467577\nmerf_r: 0.6591375725963167\n[GPBoost] [Info] Total Bins 1703\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.192076\n[GPBoost] [Info] Start training from score 2.192076\nSuper_nrmse: 0.8328671430786828\nSuper_r: 0.572070112559374\nSuper_params: {'model__C': 7, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.4, 9: 1.409090909090909, 10: 1.454545454545455, 11: 1.545454545454545, 12: 1.6, 13: 1.636363636363636, 14: 1.666666666666667, 15: 1.7, 16: 1.727272727272727, 17: 1.818181818181818, 18: 1.909090909090909, 19: 1.954545454545455, 20: 2.0, 21: 2.090909090909091, 22: 2.181818181818182, 23: 2.272727272727273, 24: 2.363636363636364, 25: 2.4, 26: 2.454545454545455, 27: 2.545454545454545, 28: 2.575, 29: 2.636363636363636, 30: 2.7, 31: 2.727272727272727, 32: 2.818181818181818, 33: 2.909090909090909, 34: 3.0, 35: 3.090909090909091, 36: 3.181818181818182, 37: 3.272727272727273, 38: 3.363636363636364, 39: 3.454545454545455, 40: 3.545454545454545, 41: 3.636363636363636, 42: 3.727272727272727, 43: 3.818181818181818, 44: 3.909090909090909, 45: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['negative', 'positive', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nTotal Time taken for featurewiz selection = 3 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\nValidation Fold: 2\n{'best_params': {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(0)}, 'best_iter': np.int64(80), 'best_score': np.float64(0.5207120563432909)}\n[GPBoost] [Info] Total Bins 1822\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.212294\n[GPBoost] [Info] Start training from score 2.212294\nGPB_nrmse: 0.6274811392456339\nGPB_r: 0.7805165458250514\nGPB_params: {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(0)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -634.303196984692 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1022.820254774151 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1131.8158632087736 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1161.2238175843772 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1158.3209943502038 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6107729550546734\nmerf_r: 0.8153336726261231\n[GPBoost] [Info] Total Bins 1822\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.212294\n[GPBoost] [Info] Start training from score 2.212294\nSuper_nrmse: 0.6280206985366223\nSuper_r: 0.7866634888046867\nSuper_params: {'model__C': 10, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.363636363636364, 7: 1.4, 8: 1.409090909090909, 9: 1.454545454545455, 10: 1.545454545454545, 11: 1.6, 12: 1.636363636363636, 13: 1.666666666666667, 14: 1.7, 15: 1.727272727272727, 16: 1.818181818181818, 17: 1.909090909090909, 18: 1.954545454545455, 19: 2.0, 20: 2.090909090909091, 21: 2.1, 22: 2.181818181818182, 23: 2.272727272727273, 24: 2.363636363636364, 25: 2.4, 26: 2.454545454545455, 27: 2.545454545454545, 28: 2.636363636363636, 29: 2.666666666666667, 30: 2.7, 31: 2.727272727272727, 32: 2.772727272727273, 33: 2.818181818181818, 34: 2.909090909090909, 35: 3.0, 36: 3.090909090909091, 37: 3.181818181818182, 38: 3.272727272727273, 39: 3.363636363636364, 40: 3.454545454545455, 41: 3.545454545454545, 42: 3.636363636363636, 43: 3.727272727272727, 44: 3.818181818181818, 45: 3.909090909090909, 46: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 15 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative', 'I_talk']\nTotal Time taken for featurewiz selection = 3 seconds\nOutput contains a list of 15 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\nValidation Fold: 3\n{'best_params': {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}, 'best_iter': np.int64(16), 'best_score': np.float64(0.48912311712833034)}\n[GPBoost] [Info] Total Bins 1752\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.168340\n[GPBoost] [Info] Start training from score 2.168340\nGPB_nrmse: 0.8850712362181739\nGPB_r: 0.6034485722935924\nGPB_params: {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -632.0955819006891 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1018.2045745546396 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1134.062942991252 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1155.87000726338 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1166.7039733281201 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6768267963299681\nmerf_r: 0.7336385759265335\n[GPBoost] [Info] Total Bins 1752\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.168340\n[GPBoost] [Info] Start training from score 2.168340\nSuper_nrmse: 0.7421411497067955\nSuper_r: 0.6935643452649219\nSuper_params: {'model__C': 10, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.181818181818182, 3: 1.272727272727273, 4: 1.3, 5: 1.363636363636364, 6: 1.4, 7: 1.409090909090909, 8: 1.454545454545455, 9: 1.545454545454545, 10: 1.6, 11: 1.636363636363636, 12: 1.666666666666667, 13: 1.7, 14: 1.727272727272727, 15: 1.818181818181818, 16: 1.909090909090909, 17: 2.0, 18: 2.090909090909091, 19: 2.1, 20: 2.181818181818182, 21: 2.272727272727273, 22: 2.363636363636364, 23: 2.454545454545455, 24: 2.545454545454545, 25: 2.575, 26: 2.636363636363636, 27: 2.666666666666667, 28: 2.7, 29: 2.727272727272727, 30: 2.772727272727273, 31: 2.818181818181818, 32: 2.909090909090909, 33: 3.0, 34: 3.090909090909091, 35: 3.181818181818182, 36: 3.272727272727273, 37: 3.363636363636364, 38: 3.454545454545455, 39: 3.545454545454545, 40: 3.636363636363636, 41: 3.727272727272727, 42: 3.818181818181818, 43: 3.909090909090909, 44: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['negative', 'positive', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative', 'I_talk']\nTotal Time taken for featurewiz selection = 3 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\nValidation Fold: 4\n{'best_params': {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}, 'best_iter': np.int64(85), 'best_score': np.float64(0.5111154896342598)}\n[GPBoost] [Info] Total Bins 1820\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.195334\n[GPBoost] [Info] Start training from score 2.195334\nGPB_nrmse: 0.6580384823683564\nGPB_r: 0.7514884351569413\nGPB_params: {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -636.6231743867645 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1020.7805449856032 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1121.7465072374005 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1154.6699119984144 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1145.6172242389339 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6476783917921103\nmerf_r: 0.7680805009079948\n[GPBoost] [Info] Total Bins 1820\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.195334\n[GPBoost] [Info] Start training from score 2.195334\nSuper_nrmse: 0.6807486237399619\nSuper_r: 0.7409257700380943\nSuper_params: {'model__C': 3, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nmerf mean r: 0.753037023467267\nmerfmean nrmse: 0.6617779774261812\nsuper mean r: 0.7129632192434523\nsupermean nrmse: 0.70652155654373\ngpb mean r: 0.7142611998218833\ngpbmean nrmse: 0.7189231502082757\n{'merf': np.float64(0.753037023467267), 'super': np.float64(0.7129632192434523), 'gpb': np.float64(0.7142611998218833)}\n{'merf': np.float64(13.938656161053588), 'super': np.float64(9.223058010997637), 'gpb': np.float64(11.702219247697894)}\nTest fold: 8\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (381, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.363636363636364, 7: 1.454545454545455, 8: 1.545454545454545, 9: 1.636363636363636, 10: 1.666666666666667, 11: 1.7, 12: 1.727272727272727, 13: 1.818181818181818, 14: 1.909090909090909, 15: 1.954545454545455, 16: 2.0, 17: 2.090909090909091, 18: 2.1, 19: 2.181818181818182, 20: 2.272727272727273, 21: 2.363636363636364, 22: 2.4, 23: 2.454545454545455, 24: 2.545454545454545, 25: 2.636363636363636, 26: 2.666666666666667, 27: 2.727272727272727, 28: 2.772727272727273, 29: 2.818181818181818, 30: 2.909090909090909, 31: 3.0, 32: 3.090909090909091, 33: 3.181818181818182, 34: 3.272727272727273, 35: 3.363636363636364, 36: 3.454545454545455, 37: 3.545454545454545, 38: 3.636363636363636, 39: 3.727272727272727, 40: 3.818181818181818, 41: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative', 'mentalfiltering', 'I_talk']\nTotal Time taken for featurewiz selection = 3 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\nValidation Fold: 0\n{'best_params': {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(5), 'lambda_l2': np.int64(1)}, 'best_iter': np.int64(14), 'best_score': np.float64(0.5516640100323136)}\n[GPBoost] [Info] Total Bins 1814\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.194743\n[GPBoost] [Info] Start training from score 2.194743\nGPB_nrmse: 0.617701289538716\nGPB_r: 0.7914097330934284\nGPB_params: {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(5), 'lambda_l2': np.int64(1)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -621.9822733292492 at iteration 1.\nINFO     [merf.py:307] Training GLL is -999.261638377074 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1097.2515300205432 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1119.3285601087957 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1126.2734466623147 at iteration 5.\n</pre> <pre>merf_nrmse: 0.5203405736539064\nmerf_r: 0.8639142442461989\n[GPBoost] [Info] Total Bins 1814\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.194743\n[GPBoost] [Info] Start training from score 2.194743\nSuper_nrmse: 0.5490362983328309\nSuper_r: 0.8345791273119996\nSuper_params: {'model__C': 2, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (381, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.181818181818182, 3: 1.2, 4: 1.272727272727273, 5: 1.3, 6: 1.363636363636364, 7: 1.4, 8: 1.454545454545455, 9: 1.545454545454545, 10: 1.6, 11: 1.636363636363636, 12: 1.666666666666667, 13: 1.7, 14: 1.727272727272727, 15: 1.818181818181818, 16: 1.909090909090909, 17: 1.954545454545455, 18: 2.0, 19: 2.090909090909091, 20: 2.1, 21: 2.181818181818182, 22: 2.272727272727273, 23: 2.363636363636364, 24: 2.4, 25: 2.454545454545455, 26: 2.545454545454545, 27: 2.636363636363636, 28: 2.666666666666667, 29: 2.727272727272727, 30: 2.772727272727273, 31: 2.818181818181818, 32: 2.909090909090909, 33: 3.0, 34: 3.090909090909091, 35: 3.181818181818182, 36: 3.272727272727273, 37: 3.363636363636364, 38: 3.454545454545455, 39: 3.545454545454545, 40: 3.636363636363636, 41: 3.727272727272727, 42: 3.818181818181818, 43: 3.909090909090909, 44: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'positive', 'srs_ges', 'negative', 'words']\nTotal Time taken for featurewiz selection = 3 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\nValidation Fold: 1\n{'best_params': {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(1)}, 'best_iter': np.int64(53), 'best_score': np.float64(0.5055003731059257)}\n[GPBoost] [Info] Total Bins 1815\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.174509\n[GPBoost] [Info] Start training from score 2.174509\nGPB_nrmse: 0.6105924260029074\nGPB_r: 0.7929754136818604\nGPB_params: {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(1)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -633.1042994368419 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1029.68080136752 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1154.1018422265893 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1168.8689092333227 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1168.9593706915427 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6951112419359493\nmerf_r: 0.717214567855442\n[GPBoost] [Info] Total Bins 1815\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.174509\n[GPBoost] [Info] Start training from score 2.174509\nSuper_nrmse: 0.7813515594023018\nSuper_r: 0.6257256148444034\nSuper_params: {'model__C': 10, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.4, 9: 1.454545454545455, 10: 1.545454545454545, 11: 1.6, 12: 1.636363636363636, 13: 1.666666666666667, 14: 1.7, 15: 1.727272727272727, 16: 1.818181818181818, 17: 1.909090909090909, 18: 1.954545454545455, 19: 2.0, 20: 2.090909090909091, 21: 2.181818181818182, 22: 2.272727272727273, 23: 2.363636363636364, 24: 2.4, 25: 2.454545454545455, 26: 2.545454545454545, 27: 2.636363636363636, 28: 2.727272727272727, 29: 2.772727272727273, 30: 2.818181818181818, 31: 2.909090909090909, 32: 3.0, 33: 3.090909090909091, 34: 3.181818181818182, 35: 3.272727272727273, 36: 3.363636363636364, 37: 3.454545454545455, 38: 3.545454545454545, 39: 3.636363636363636, 40: 3.727272727272727, 41: 3.818181818181818, 42: 3.909090909090909, 43: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'I_talk', 'words']\nTotal Time taken for featurewiz selection = 3 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\nValidation Fold: 2\n{'best_params': {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(5), 'lambda_l2': np.int64(0)}, 'best_iter': np.int64(11), 'best_score': np.float64(0.46168993929133484)}\n[GPBoost] [Info] Total Bins 1823\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.182540\n[GPBoost] [Info] Start training from score 2.182540\nGPB_nrmse: 0.907671035629425\nGPB_r: 0.5641389673792572\nGPB_params: {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(5), 'lambda_l2': np.int64(0)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -641.2346087870469 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1052.9487804909866 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1193.0517103868879 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1228.046891448212 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1223.4784489950969 at iteration 5.\n</pre> <pre>merf_nrmse: 0.8112438855239015\nmerf_r: 0.6110466607463946\n[GPBoost] [Info] Total Bins 1823\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.182540\n[GPBoost] [Info] Start training from score 2.182540\nSuper_nrmse: 0.8238771934281109\nSuper_r: 0.6130565811748856\nSuper_params: {'model__C': 10, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'scale', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.4, 9: 1.454545454545455, 10: 1.545454545454545, 11: 1.6, 12: 1.636363636363636, 13: 1.666666666666667, 14: 1.7, 15: 1.727272727272727, 16: 1.818181818181818, 17: 1.909090909090909, 18: 1.954545454545455, 19: 2.0, 20: 2.090909090909091, 21: 2.1, 22: 2.181818181818182, 23: 2.272727272727273, 24: 2.363636363636364, 25: 2.454545454545455, 26: 2.545454545454545, 27: 2.636363636363636, 28: 2.666666666666667, 29: 2.727272727272727, 30: 2.818181818181818, 31: 2.909090909090909, 32: 3.0, 33: 3.090909090909091, 34: 3.181818181818182, 35: 3.272727272727273, 36: 3.363636363636364, 37: 3.454545454545455, 38: 3.545454545454545, 39: 3.636363636363636, 40: 3.727272727272727, 41: 3.818181818181818, 42: 3.909090909090909, 43: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['negative', 'positive', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'positive', 'srs_ges', 'negative', 'words']\nTotal Time taken for featurewiz selection = 3 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\nValidation Fold: 3\n{'best_params': {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(1)}, 'best_iter': np.int64(188), 'best_score': np.float64(0.5272239213554453)}\n[GPBoost] [Info] Total Bins 1812\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.176955\n[GPBoost] [Info] Start training from score 2.176955\nGPB_nrmse: 0.5855212185242716\nGPB_r: 0.8138155282959905\nGPB_params: {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(1)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -628.433103542199 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1021.653783593487 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1141.238313828004 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1171.1919464837792 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1170.688350021997 at iteration 5.\n</pre> <pre>merf_nrmse: 0.5957398840538508\nmerf_r: 0.8089275278773507\n[GPBoost] [Info] Total Bins 1812\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 16\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.176955\n[GPBoost] [Info] Start training from score 2.176955\nSuper_nrmse: 0.6345701855234236\nSuper_r: 0.7726146613954724\nSuper_params: {'model__C': 5, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.272727272727273, 5: 1.3, 6: 1.363636363636364, 7: 1.4, 8: 1.454545454545455, 9: 1.545454545454545, 10: 1.6, 11: 1.636363636363636, 12: 1.727272727272727, 13: 1.818181818181818, 14: 1.909090909090909, 15: 2.0, 16: 2.090909090909091, 17: 2.1, 18: 2.181818181818182, 19: 2.272727272727273, 20: 2.363636363636364, 21: 2.4, 22: 2.454545454545455, 23: 2.545454545454545, 24: 2.636363636363636, 25: 2.666666666666667, 26: 2.727272727272727, 27: 2.772727272727273, 28: 2.818181818181818, 29: 2.909090909090909, 30: 3.0, 31: 3.090909090909091, 32: 3.181818181818182, 33: 3.272727272727273, 34: 3.363636363636364, 35: 3.454545454545455, 36: 3.545454545454545, 37: 3.636363636363636, 38: 3.727272727272727, 39: 3.818181818181818, 40: 3.909090909090909, 41: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (2) highly correlated variables:\n    ['I_talk', 'neutral']\n    Following (18) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\nCompleted SULOV. 18 features selected\nTime taken for SULOV method = 0 seconds\nFinally 18 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 15 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\nTotal Time taken for featurewiz selection = 3 seconds\nOutput contains a list of 15 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\nValidation Fold: 4\n{'best_params': {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(5), 'lambda_l2': np.int64(10)}, 'best_iter': np.int64(1), 'best_score': np.float64(0.5114468192971784)}\n[GPBoost] [Info] Total Bins 1699\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.220784\n[GPBoost] [Info] Start training from score 2.220784\nGPB_nrmse: 2.1604080569750996\nGPB_r: -0.036927770376247546\nGPB_params: {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(5), 'lambda_l2': np.int64(10)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -632.5698496348056 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1020.0054970278792 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1139.0085671315635 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1160.8757049065086 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1165.1052394913736 at iteration 5.\n</pre> <pre>merf_nrmse: 0.7222619938532552\nmerf_r: 0.7171271211430358\n[GPBoost] [Info] Total Bins 1699\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.220784\n[GPBoost] [Info] Start training from score 2.220784\nSuper_nrmse: 0.9126107206024018\nSuper_r: 0.5302787304693927\nSuper_params: {'model__C': 2, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nmerf mean r: 0.7436460243736844\nmerfmean nrmse: 0.6689395158041727\nsuper mean r: 0.6752509430392306\nsupermean nrmse: 0.7402891914578138\ngpb mean r: 0.5850823744148579\ngpbmean nrmse: 0.9763788053340839\n{'merf': np.float64(0.7436460243736844), 'super': np.float64(0.6752509430392306), 'gpb': np.float64(0.5850823744148579)}\n{'merf': np.float64(8.562875491037902), 'super': np.float64(6.054029392115507), 'gpb': np.float64(1.8048393539891752)}\nTest fold: 9\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (381, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.363636363636364, 7: 1.409090909090909, 8: 1.454545454545455, 9: 1.545454545454545, 10: 1.636363636363636, 11: 1.666666666666667, 12: 1.7, 13: 1.727272727272727, 14: 1.818181818181818, 15: 1.909090909090909, 16: 2.0, 17: 2.090909090909091, 18: 2.1, 19: 2.181818181818182, 20: 2.272727272727273, 21: 2.363636363636364, 22: 2.4, 23: 2.454545454545455, 24: 2.545454545454545, 25: 2.575, 26: 2.636363636363636, 27: 2.666666666666667, 28: 2.7, 29: 2.727272727272727, 30: 2.772727272727273, 31: 2.818181818181818, 32: 2.909090909090909, 33: 3.0, 34: 3.090909090909091, 35: 3.181818181818182, 36: 3.272727272727273, 37: 3.363636363636364, 38: 3.454545454545455, 39: 3.545454545454545, 40: 3.636363636363636, 41: 3.727272727272727, 42: 3.818181818181818, 43: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (2) highly correlated variables:\n    ['I_talk', 'neutral']\n    Following (18) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\nCompleted SULOV. 18 features selected\nTime taken for SULOV method = 0 seconds\nFinally 18 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 15 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative', 'mentalfiltering']\nTotal Time taken for featurewiz selection = 3 seconds\nOutput contains a list of 15 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\nValidation Fold: 0\n{'best_params': {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(5), 'lambda_l2': np.int64(10)}, 'best_iter': np.int64(124), 'best_score': np.float64(0.5212983438823757)}\n[GPBoost] [Info] Total Bins 1693\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.190454\n[GPBoost] [Info] Start training from score 2.190454\nGPB_nrmse: 0.5819096729366113\nGPB_r: 0.8111420038736332\nGPB_params: {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(5), 'lambda_l2': np.int64(10)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -626.2389349347427 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1015.0277003821353 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1119.6468097592908 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1146.9227692123827 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1127.9996259975633 at iteration 5.\n</pre> <pre>merf_nrmse: 0.5874811511848783\nmerf_r: 0.8113046790527997\n[GPBoost] [Info] Total Bins 1693\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.190454\n[GPBoost] [Info] Start training from score 2.190454\nSuper_nrmse: 0.6289959666503276\nSuper_r: 0.7788238388530284\nSuper_params: {'model__C': 7, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'scale', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (381, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.181818181818182, 3: 1.2, 4: 1.272727272727273, 5: 1.3, 6: 1.363636363636364, 7: 1.409090909090909, 8: 1.454545454545455, 9: 1.545454545454545, 10: 1.636363636363636, 11: 1.666666666666667, 12: 1.7, 13: 1.727272727272727, 14: 1.818181818181818, 15: 1.909090909090909, 16: 1.954545454545455, 17: 2.0, 18: 2.090909090909091, 19: 2.181818181818182, 20: 2.272727272727273, 21: 2.363636363636364, 22: 2.454545454545455, 23: 2.545454545454545, 24: 2.575, 25: 2.636363636363636, 26: 2.666666666666667, 27: 2.7, 28: 2.727272727272727, 29: 2.818181818181818, 30: 2.909090909090909, 31: 3.0, 32: 3.090909090909091, 33: 3.181818181818182, 34: 3.272727272727273, 35: 3.363636363636364, 36: 3.454545454545455, 37: 3.545454545454545, 38: 3.636363636363636, 39: 3.727272727272727, 40: 3.818181818181818, 41: 3.909090909090909, 42: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (2) highly correlated variables:\n    ['I_talk', 'neutral']\n    Following (18) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\nCompleted SULOV. 18 features selected\nTime taken for SULOV method = 0 seconds\nFinally 18 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 15 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\nTotal Time taken for featurewiz selection = 3 seconds\nOutput contains a list of 15 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\nValidation Fold: 1\n{'best_params': {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}, 'best_iter': np.int64(20), 'best_score': np.float64(0.5221161327382114)}\n[GPBoost] [Info] Total Bins 1685\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.184011\n[GPBoost] [Info] Start training from score 2.184011\nGPB_nrmse: 0.720040401998447\nGPB_r: 0.7006290535262134\nGPB_params: {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -629.9645176369414 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1005.709216482386 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1103.0064236462188 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1117.9695373737245 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1115.3109655278733 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6498399713802036\nmerf_r: 0.7584044697857344\n[GPBoost] [Info] Total Bins 1685\n[GPBoost] [Info] Number of data points in the train set: 381, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.184011\n[GPBoost] [Info] Start training from score 2.184011\nSuper_nrmse: 0.6387099059837215\nSuper_r: 0.7681913962986885\nSuper_params: {'model__C': 9, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'scale', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.409090909090909, 9: 1.454545454545455, 10: 1.545454545454545, 11: 1.636363636363636, 12: 1.666666666666667, 13: 1.7, 14: 1.727272727272727, 15: 1.818181818181818, 16: 1.909090909090909, 17: 1.954545454545455, 18: 2.0, 19: 2.090909090909091, 20: 2.1, 21: 2.181818181818182, 22: 2.272727272727273, 23: 2.363636363636364, 24: 2.4, 25: 2.454545454545455, 26: 2.545454545454545, 27: 2.636363636363636, 28: 2.7, 29: 2.727272727272727, 30: 2.772727272727273, 31: 2.818181818181818, 32: 2.909090909090909, 33: 3.0, 34: 3.090909090909091, 35: 3.181818181818182, 36: 3.272727272727273, 37: 3.363636363636364, 38: 3.454545454545455, 39: 3.545454545454545, 40: 3.636363636363636, 41: 3.727272727272727, 42: 3.818181818181818, 43: 3.909090909090909, 44: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (2) highly correlated variables:\n    ['I_talk', 'neutral']\n    Following (18) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\nCompleted SULOV. 18 features selected\nTime taken for SULOV method = 0 seconds\nFinally 18 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['words', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 15 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'words', 'negative', 'mentalfiltering']\nTotal Time taken for featurewiz selection = 4 seconds\nOutput contains a list of 15 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\nValidation Fold: 2\n{'best_params': {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}, 'best_iter': np.int64(116), 'best_score': np.float64(0.4935886319928947)}\n[GPBoost] [Info] Total Bins 1696\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.228375\n[GPBoost] [Info] Start training from score 2.228375\nGPB_nrmse: 0.7999860521094719\nGPB_r: 0.6653794825170457\nGPB_params: {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(100), 'max_depth': np.int64(2), 'lambda_l2': np.int64(10)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -638.6942917957987 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1038.0052833918141 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1151.1265930398176 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1183.1046754552108 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1184.7859574548495 at iteration 5.\n</pre> <pre>merf_nrmse: 0.8758297178701928\nmerf_r: 0.5753820632356995\n[GPBoost] [Info] Total Bins 1696\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 15\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.228375\n[GPBoost] [Info] Start training from score 2.228375\nSuper_nrmse: 0.9706917509308552\nSuper_r: 0.485042954543172\nSuper_params: {'model__C': 3, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.454545454545455, 9: 1.545454545454545, 10: 1.636363636363636, 11: 1.666666666666667, 12: 1.727272727272727, 13: 1.818181818181818, 14: 1.909090909090909, 15: 1.954545454545455, 16: 2.0, 17: 2.090909090909091, 18: 2.1, 19: 2.181818181818182, 20: 2.272727272727273, 21: 2.363636363636364, 22: 2.4, 23: 2.454545454545455, 24: 2.545454545454545, 25: 2.575, 26: 2.636363636363636, 27: 2.666666666666667, 28: 2.7, 29: 2.727272727272727, 30: 2.772727272727273, 31: 2.818181818181818, 32: 2.909090909090909, 33: 3.0, 34: 3.090909090909091, 35: 3.181818181818182, 36: 3.272727272727273, 37: 3.363636363636364, 38: 3.454545454545455, 39: 3.545454545454545, 40: 3.636363636363636, 41: 3.727272727272727, 42: 3.818181818181818, 43: 3.909090909090909, 44: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (2) highly correlated variables:\n    ['I_talk', 'neutral']\n    Following (18) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\nCompleted SULOV. 18 features selected\nTime taken for SULOV method = 0 seconds\nFinally 18 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 14 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words', 'minimization']\nTotal Time taken for featurewiz selection = 3 seconds\nOutput contains a list of 14 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\nValidation Fold: 3\n{'best_params': {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(1)}, 'best_iter': np.int64(84), 'best_score': np.float64(0.502930109991378)}\n[GPBoost] [Info] Total Bins 1629\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 14\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.193977\n[GPBoost] [Info] Start training from score 2.193977\nGPB_nrmse: 0.6301515982926553\nGPB_r: 0.776012883004224\nGPB_params: {'learning_rate': np.float64(0.01), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(2), 'lambda_l2': np.int64(1)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -637.8066191816966 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1023.6448158942071 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1133.5493050449584 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1154.6359587475263 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1161.3908075542809 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6804840168208731\nmerf_r: 0.7339139606581573\n[GPBoost] [Info] Total Bins 1629\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 14\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.193977\n[GPBoost] [Info] Start training from score 2.193977\nSuper_nrmse: 0.7554573614343177\nSuper_r: 0.657074824576692\nSuper_params: {'model__C': 2, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (382, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.272727272727273, 5: 1.3, 6: 1.363636363636364, 7: 1.409090909090909, 8: 1.454545454545455, 9: 1.545454545454545, 10: 1.636363636363636, 11: 1.7, 12: 1.727272727272727, 13: 1.818181818181818, 14: 1.909090909090909, 15: 1.954545454545455, 16: 2.0, 17: 2.090909090909091, 18: 2.1, 19: 2.181818181818182, 20: 2.272727272727273, 21: 2.363636363636364, 22: 2.4, 23: 2.454545454545455, 24: 2.545454545454545, 25: 2.575, 26: 2.636363636363636, 27: 2.666666666666667, 28: 2.727272727272727, 29: 2.772727272727273, 30: 2.818181818181818, 31: 2.909090909090909, 32: 3.0, 33: 3.090909090909091, 34: 3.181818181818182, 35: 3.272727272727273, 36: 3.363636363636364, 37: 3.454545454545455, 38: 3.545454545454545, 39: 3.636363636363636, 40: 3.727272727272727, 41: 3.818181818181818, 42: 3.909090909090909, 43: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (2) highly correlated variables:\n    ['I_talk', 'neutral']\n    Following (18) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\nCompleted SULOV. 18 features selected\nTime taken for SULOV method = 0 seconds\nFinally 18 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 14 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\nTotal Time taken for featurewiz selection = 3 seconds\nOutput contains a list of 14 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\nValidation Fold: 4\n{'best_params': {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(5), 'lambda_l2': np.int64(10)}, 'best_iter': np.int64(4), 'best_score': np.float64(0.5404005826539976)}\n[GPBoost] [Info] Total Bins 1626\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 14\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.204916\n[GPBoost] [Info] Start training from score 2.204916\nGPB_nrmse: 1.5456198705922426\nGPB_r: 0.2732594930474329\nGPB_params: {'learning_rate': np.float64(0.1), 'min_data_in_leaf': np.int64(10), 'max_depth': np.int64(5), 'lambda_l2': np.int64(10)}\n</pre> <pre>INFO     [merf.py:307] Training GLL is -628.0814040462756 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1009.8373361342933 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1117.4449434078622 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1129.1528251994775 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1130.3037649667983 at iteration 5.\n</pre> <pre>merf_nrmse: 0.6340105451676304\nmerf_r: 0.7887859520571678\n[GPBoost] [Info] Total Bins 1626\n[GPBoost] [Info] Number of data points in the train set: 382, number of used features: 14\n[GPBoost] [Info] [GPBoost with gaussian likelihood]: initscore=2.204916\n[GPBoost] [Info] Start training from score 2.204916\nSuper_nrmse: 0.7699501237887033\nSuper_r: 0.6358644847155\nSuper_params: {'model__C': 2, 'model__coef0': 1e-20, 'model__degree': 2, 'model__gamma': 'auto', 'model__kernel': 'rbf'}\nmerf mean r: 0.7335582249579117\nmerfmean nrmse: 0.6855290804847556\nsuper mean r: 0.6649994997974161\nsupermean nrmse: 0.7527610217575852\ngpb mean r: 0.6452845831937098\ngpbmean nrmse: 0.8555415191858856\n{'merf': np.float64(0.7335582249579117), 'super': np.float64(0.6649994997974161), 'gpb': np.float64(0.6452845831937098)}\n{'merf': np.float64(8.801488236068995), 'super': np.float64(6.233731149518636), 'gpb': np.float64(3.341443103250428)}\nTest fold: fold_0\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (477, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.4, 9: 1.409090909090909, 10: 1.454545454545455, 11: 1.545454545454545, 12: 1.6, 13: 1.636363636363636, 14: 1.666666666666667, 15: 1.7, 16: 1.727272727272727, 17: 1.818181818181818, 18: 1.909090909090909, 19: 1.954545454545455, 20: 2.0, 21: 2.090909090909091, 22: 2.181818181818182, 23: 2.272727272727273, 24: 2.363636363636364, 25: 2.4, 26: 2.454545454545455, 27: 2.545454545454545, 28: 2.575, 29: 2.636363636363636, 30: 2.666666666666667, 31: 2.7, 32: 2.727272727272727, 33: 2.772727272727273, 34: 2.818181818181818, 35: 2.909090909090909, 36: 3.0, 37: 3.090909090909091, 38: 3.181818181818182, 39: 3.272727272727273, 40: 3.363636363636364, 41: 3.454545454545455, 42: 3.545454545454545, 43: 3.636363636363636, 44: 3.727272727272727, 45: 3.818181818181818, 46: 3.909090909090909, 47: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative', 'I_talk']\nTotal Time taken for featurewiz selection = 4 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 4 second(s)\n</pre> <pre>INFO     [merf.py:307] Training GLL is -830.346887363139 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1319.6464936915086 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1445.3079533738924 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1477.1669310948243 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1463.4226360866942 at iteration 5.\n</pre> <pre>Auswahl: merf: 0.7593365442461725\nTest fold: fold_1\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (477, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.4, 9: 1.409090909090909, 10: 1.454545454545455, 11: 1.545454545454545, 12: 1.6, 13: 1.636363636363636, 14: 1.7, 15: 1.727272727272727, 16: 1.818181818181818, 17: 1.909090909090909, 18: 2.0, 19: 2.090909090909091, 20: 2.1, 21: 2.181818181818182, 22: 2.272727272727273, 23: 2.363636363636364, 24: 2.4, 25: 2.454545454545455, 26: 2.545454545454545, 27: 2.575, 28: 2.636363636363636, 29: 2.666666666666667, 30: 2.7, 31: 2.727272727272727, 32: 2.772727272727273, 33: 2.818181818181818, 34: 2.909090909090909, 35: 3.0, 36: 3.090909090909091, 37: 3.181818181818182, 38: 3.272727272727273, 39: 3.363636363636364, 40: 3.454545454545455, 41: 3.545454545454545, 42: 3.636363636363636, 43: 3.727272727272727, 44: 3.818181818181818, 45: 3.909090909090909, 46: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative', 'I_talk']\nTotal Time taken for featurewiz selection = 4 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\n</pre> <pre>INFO     [merf.py:307] Training GLL is -829.7811066697792 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1309.0425266459583 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1419.1051116975661 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1444.6275776894608 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1442.89763038657 at iteration 5.\n</pre> <pre>Auswahl: merf: 0.8379454117093825\nTest fold: fold_2\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (477, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.181818181818182, 3: 1.2, 4: 1.272727272727273, 5: 1.3, 6: 1.363636363636364, 7: 1.4, 8: 1.409090909090909, 9: 1.454545454545455, 10: 1.545454545454545, 11: 1.6, 12: 1.636363636363636, 13: 1.666666666666667, 14: 1.7, 15: 1.727272727272727, 16: 1.818181818181818, 17: 1.909090909090909, 18: 1.954545454545455, 19: 2.0, 20: 2.090909090909091, 21: 2.1, 22: 2.181818181818182, 23: 2.272727272727273, 24: 2.363636363636364, 25: 2.4, 26: 2.454545454545455, 27: 2.545454545454545, 28: 2.575, 29: 2.636363636363636, 30: 2.666666666666667, 31: 2.7, 32: 2.727272727272727, 33: 2.772727272727273, 34: 2.818181818181818, 35: 2.909090909090909, 36: 3.0, 37: 3.090909090909091, 38: 3.181818181818182, 39: 3.272727272727273, 40: 3.363636363636364, 41: 3.454545454545455, 42: 3.545454545454545, 43: 3.636363636363636, 44: 3.727272727272727, 45: 3.818181818181818, 46: 3.909090909090909, 47: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['negative', 'positive', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative', 'I_talk']\nTotal Time taken for featurewiz selection = 4 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 4 second(s)\n</pre> <pre>INFO     [merf.py:307] Training GLL is -829.959144057881 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1303.2672541013796 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1412.970457261338 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1435.0021950770533 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1432.5116069555752 at iteration 5.\n</pre> <pre>Auswahl: merf: 0.7660119612148696\nTest fold: fold_3\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (477, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.272727272727273, 5: 1.3, 6: 1.363636363636364, 7: 1.4, 8: 1.409090909090909, 9: 1.454545454545455, 10: 1.545454545454545, 11: 1.6, 12: 1.636363636363636, 13: 1.666666666666667, 14: 1.7, 15: 1.727272727272727, 16: 1.818181818181818, 17: 1.909090909090909, 18: 1.954545454545455, 19: 2.0, 20: 2.090909090909091, 21: 2.1, 22: 2.181818181818182, 23: 2.272727272727273, 24: 2.363636363636364, 25: 2.454545454545455, 26: 2.545454545454545, 27: 2.575, 28: 2.636363636363636, 29: 2.666666666666667, 30: 2.7, 31: 2.727272727272727, 32: 2.772727272727273, 33: 2.818181818181818, 34: 2.909090909090909, 35: 3.0, 36: 3.090909090909091, 37: 3.181818181818182, 38: 3.272727272727273, 39: 3.363636363636364, 40: 3.454545454545455, 41: 3.545454545454545, 42: 3.636363636363636, 43: 3.727272727272727, 44: 3.818181818181818, 45: 3.909090909090909, 46: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['negative', 'positive', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 15 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative', 'I_talk']\nTotal Time taken for featurewiz selection = 4 seconds\nOutput contains a list of 15 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\n</pre> <pre>INFO     [merf.py:307] Training GLL is -822.4226419805485 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1295.8186274489544 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1416.4316797871504 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1431.0325922027823 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1433.6030296531012 at iteration 5.\n</pre> <pre>Auswahl: merf: 0.8022894299782736\nTest fold: fold_4\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (477, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.4, 9: 1.409090909090909, 10: 1.454545454545455, 11: 1.545454545454545, 12: 1.6, 13: 1.636363636363636, 14: 1.666666666666667, 15: 1.727272727272727, 16: 1.818181818181818, 17: 1.909090909090909, 18: 1.954545454545455, 19: 2.0, 20: 2.090909090909091, 21: 2.1, 22: 2.181818181818182, 23: 2.272727272727273, 24: 2.363636363636364, 25: 2.4, 26: 2.454545454545455, 27: 2.545454545454545, 28: 2.575, 29: 2.636363636363636, 30: 2.666666666666667, 31: 2.7, 32: 2.727272727272727, 33: 2.772727272727273, 34: 2.818181818181818, 35: 2.909090909090909, 36: 3.0, 37: 3.090909090909091, 38: 3.181818181818182, 39: 3.272727272727273, 40: 3.363636363636364, 41: 3.454545454545455, 42: 3.545454545454545, 43: 3.636363636363636, 44: 3.727272727272727, 45: 3.818181818181818, 46: 3.909090909090909, 47: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 15 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative', 'fortunetelling', 'I_talk']\nTotal Time taken for featurewiz selection = 4 seconds\nOutput contains a list of 15 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\n</pre> <pre>INFO     [merf.py:307] Training GLL is -821.3287142379637 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1290.9426213959218 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1402.448239772095 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1422.2662761334655 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1434.2389943512476 at iteration 5.\n</pre> <pre>Auswahl: merf: 0.7641876186183926\nTest fold: fold_5\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (477, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.4, 9: 1.409090909090909, 10: 1.454545454545455, 11: 1.545454545454545, 12: 1.6, 13: 1.636363636363636, 14: 1.666666666666667, 15: 1.7, 16: 1.727272727272727, 17: 1.818181818181818, 18: 1.909090909090909, 19: 1.954545454545455, 20: 2.0, 21: 2.090909090909091, 22: 2.1, 23: 2.181818181818182, 24: 2.272727272727273, 25: 2.363636363636364, 26: 2.4, 27: 2.454545454545455, 28: 2.545454545454545, 29: 2.575, 30: 2.636363636363636, 31: 2.666666666666667, 32: 2.7, 33: 2.727272727272727, 34: 2.772727272727273, 35: 2.818181818181818, 36: 2.909090909090909, 37: 3.0, 38: 3.090909090909091, 39: 3.181818181818182, 40: 3.272727272727273, 41: 3.363636363636364, 42: 3.454545454545455, 43: 3.545454545454545, 44: 3.636363636363636, 45: 3.727272727272727, 46: 3.818181818181818, 47: 3.909090909090909, 48: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['negative', 'positive', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative', 'mentalfiltering']\nTotal Time taken for featurewiz selection = 4 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\n</pre> <pre>INFO     [merf.py:307] Training GLL is -830.9691217311275 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1298.3689852549064 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1415.9069669963353 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1445.329140599526 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1426.6251692622402 at iteration 5.\n</pre> <pre>Auswahl: merf: 0.6907765622333976\nTest fold: fold_6\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (477, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.4, 9: 1.409090909090909, 10: 1.454545454545455, 11: 1.545454545454545, 12: 1.6, 13: 1.636363636363636, 14: 1.666666666666667, 15: 1.7, 16: 1.727272727272727, 17: 1.818181818181818, 18: 1.909090909090909, 19: 1.954545454545455, 20: 2.0, 21: 2.090909090909091, 22: 2.1, 23: 2.181818181818182, 24: 2.272727272727273, 25: 2.363636363636364, 26: 2.4, 27: 2.454545454545455, 28: 2.545454545454545, 29: 2.575, 30: 2.636363636363636, 31: 2.7, 32: 2.727272727272727, 33: 2.818181818181818, 34: 2.909090909090909, 35: 3.0, 36: 3.090909090909091, 37: 3.181818181818182, 38: 3.272727272727273, 39: 3.363636363636364, 40: 3.454545454545455, 41: 3.545454545454545, 42: 3.636363636363636, 43: 3.727272727272727, 44: 3.818181818181818, 45: 3.909090909090909, 46: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative', 'mentalfiltering']\nTotal Time taken for featurewiz selection = 4 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\n</pre> <pre>INFO     [merf.py:307] Training GLL is -831.9327664896133 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1307.5042065593082 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1437.2463031790644 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1468.49865088685 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1467.3224120740022 at iteration 5.\n</pre> <pre>Auswahl: merf: 0.7294905287723659\nTest fold: fold_7\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (477, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.4, 9: 1.409090909090909, 10: 1.454545454545455, 11: 1.545454545454545, 12: 1.6, 13: 1.636363636363636, 14: 1.666666666666667, 15: 1.7, 16: 1.727272727272727, 17: 1.818181818181818, 18: 1.909090909090909, 19: 1.954545454545455, 20: 2.0, 21: 2.090909090909091, 22: 2.1, 23: 2.181818181818182, 24: 2.272727272727273, 25: 2.363636363636364, 26: 2.4, 27: 2.454545454545455, 28: 2.545454545454545, 29: 2.575, 30: 2.636363636363636, 31: 2.666666666666667, 32: 2.7, 33: 2.727272727272727, 34: 2.772727272727273, 35: 2.818181818181818, 36: 2.909090909090909, 37: 3.0, 38: 3.090909090909091, 39: 3.181818181818182, 40: 3.272727272727273, 41: 3.363636363636364, 42: 3.454545454545455, 43: 3.545454545454545, 44: 3.636363636363636, 45: 3.727272727272727, 46: 3.818181818181818, 47: 3.909090909090909, 48: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative', 'I_talk']\nTotal Time taken for featurewiz selection = 4 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\n</pre> <pre>INFO     [merf.py:307] Training GLL is -827.0093553368508 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1301.195942732204 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1406.0803534921638 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1414.5086807652015 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1424.2550357916077 at iteration 5.\n</pre> <pre>Auswahl: merf: 0.8010665891389369\nTest fold: fold_8\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (477, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.4, 9: 1.454545454545455, 10: 1.545454545454545, 11: 1.6, 12: 1.636363636363636, 13: 1.666666666666667, 14: 1.7, 15: 1.727272727272727, 16: 1.818181818181818, 17: 1.909090909090909, 18: 1.954545454545455, 19: 2.0, 20: 2.090909090909091, 21: 2.1, 22: 2.181818181818182, 23: 2.272727272727273, 24: 2.363636363636364, 25: 2.4, 26: 2.454545454545455, 27: 2.545454545454545, 28: 2.636363636363636, 29: 2.666666666666667, 30: 2.727272727272727, 31: 2.772727272727273, 32: 2.818181818181818, 33: 2.909090909090909, 34: 3.0, 35: 3.090909090909091, 36: 3.181818181818182, 37: 3.272727272727273, 38: 3.363636363636364, 39: 3.454545454545455, 40: 3.545454545454545, 41: 3.636363636363636, 42: 3.727272727272727, 43: 3.818181818181818, 44: 3.909090909090909, 45: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (1) highly correlated variables:\n    ['neutral']\n    Following (19) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\nCompleted SULOV. 19 features selected\nTime taken for SULOV method = 0 seconds\nFinally 19 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'I_talk', 'words', 'positive', 'srs_ges', 'negative']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['positive', 'negative', 'srs_ges']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 16 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words', 'I_talk']\nTotal Time taken for featurewiz selection = 4 seconds\nOutput contains a list of 16 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\n</pre> <pre>INFO     [merf.py:307] Training GLL is -818.1291378846489 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1292.843207298202 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1410.2888491700164 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1427.2667031518563 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1428.691995738558 at iteration 5.\n</pre> <pre>Auswahl: merf: 0.8302349413303363\nTest fold: fold_9\nfeaturewiz has selected 0.8 as the correlation limit. Change this limit to fit your needs...\nSkipping feature engineering since no feature_engg input...\nSkipping category encoding since no category encoders specified in input...\n#### Single_Label Multi_Classification problem ####\n    Loaded train data. Shape = (477, 21)\n    Some column names had special characters which were removed...\n#### Single_Label Multi_Classification problem ####\nNo test data filename given...\n#######################################################################################\n######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n#######################################################################################\n        No variables were removed since no ID or low-information variables found in data set\nRemoving 0 columns from further processing since ID or low information variables\n    target labels need to be converted...\nCompleted label encoding of target variable = hscl\nHow model predictions need to be transformed for hscl:\n\t{0: 1.0, 1: 1.090909090909091, 2: 1.1, 3: 1.181818181818182, 4: 1.2, 5: 1.272727272727273, 6: 1.3, 7: 1.363636363636364, 8: 1.409090909090909, 9: 1.454545454545455, 10: 1.545454545454545, 11: 1.636363636363636, 12: 1.666666666666667, 13: 1.7, 14: 1.727272727272727, 15: 1.818181818181818, 16: 1.909090909090909, 17: 1.954545454545455, 18: 2.0, 19: 2.090909090909091, 20: 2.1, 21: 2.181818181818182, 22: 2.272727272727273, 23: 2.363636363636364, 24: 2.4, 25: 2.454545454545455, 26: 2.545454545454545, 27: 2.575, 28: 2.636363636363636, 29: 2.666666666666667, 30: 2.7, 31: 2.727272727272727, 32: 2.772727272727273, 33: 2.818181818181818, 34: 2.909090909090909, 35: 3.0, 36: 3.090909090909091, 37: 3.181818181818182, 38: 3.272727272727273, 39: 3.363636363636364, 40: 3.454545454545455, 41: 3.545454545454545, 42: 3.636363636363636, 43: 3.727272727272727, 44: 3.818181818181818, 45: 3.909090909090909, 46: 4.0}\n#######################################################################################\n#####  Searching for Uncorrelated List Of Variables (SULOV) in 20 features ############\n#######################################################################################\n    there are no null values in dataset...\n    Removing (2) highly correlated variables:\n    ['I_talk', 'neutral']\n    Following (18) vars selected: ['comparison', 'labelingandmislabeling', 'catastrophizing', 'dichotomousreasoning', 'emotionalreasoning', 'disqualifyingthepositive', 'magnification', 'minimization', 'mentalfiltering', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\nCompleted SULOV. 18 features selected\nTime taken for SULOV method = 0 seconds\nFinally 18 vars selected after SULOV\nConverting all features to numeric before sending to XGBoost...\n    Number of booster rounds = 100\n        Selected: ['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n        Selected: ['negative', 'words']\n            Time taken for regular XGBoost feature selection = 1 seconds\n    Completed XGBoost feature selection in 1 seconds\nSelected 14 important features:\n['comparison', 'labelingandmislabeling', 'dichotomousreasoning', 'disqualifyingthepositive', 'minimization', 'mindreading', 'fortunetelling', 'overgeneralizing', 'personalizing', 'shouldstatements', 'positive', 'srs_ges', 'negative', 'words']\nTotal Time taken for featurewiz selection = 4 seconds\nOutput contains a list of 14 important features and a train dataframe\n    Time taken to create entire pipeline = 3 second(s)\n</pre> <pre>INFO     [merf.py:307] Training GLL is -821.7869244203891 at iteration 1.\nINFO     [merf.py:307] Training GLL is -1280.0766921017182 at iteration 2.\nINFO     [merf.py:307] Training GLL is -1391.176681991686 at iteration 3.\nINFO     [merf.py:307] Training GLL is -1419.6559510316806 at iteration 4.\nINFO     [merf.py:307] Training GLL is -1404.5587355262167 at iteration 5.\n</pre> <pre>Auswahl: merf: 0.8572965178097764\n</pre> Out[\u00a0]: <pre>0</pre> <pre>&lt;Figure size 640x480 with 0 Axes&gt;</pre>"},{"location":"BasicUsage/#project-descriptions","title":"Project descriptions\u00b6","text":"<p>X-RAI (eXplanable Regression-based Artificial Intelligence) is a package for the integration of regression-based machine-learning and eXplainable AI via the SHAP-package. X-RAI allows the prediction of a target variable via several ML algorithms from scikit-learn (Lasso, Elastic Net, Random Forest, Support Vector Regression, XGBoost, and a Support Vector Regression meta-learner). Algorithm selection is conducted via nested cross-validation.</p> <p>Specifications can be selected regarding the internal and external folds, the selected algorithms and the use of feature selection via featureWiz. The cross-validation scheme can be safed for reproductibility. SHAP allows the estimation of feature importance both for individual predictions, as well as across the whole dataset.</p>"}]}